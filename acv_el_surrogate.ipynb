{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21dfdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\r\n",
      "Version: 1.20.0\r\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\r\n",
      "Home-page: https://www.numpy.org\r\n",
      "Author: Travis E. Oliphant et al.\r\n",
      "Author-email: \r\n",
      "License: BSD\r\n",
      "Location: /home/mythreyi/.local/lib/python3.9/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: acv-exp, altair, anchor-exp, blis, catboost, h5py, hyperopt, imageio, imodels, Keras-Preprocessing, lightgbm, lime, matplotlib, mlxtend, numba, opt-einsum, pandas, pyagrum, pyarrow, pydeck, PyWavelets, scikit-image, scikit-learn, scipy, seaborn, shap, spacy, streamlit, tensorboard, tensorflow, thinc, tifffile, xgboost\r\n"
     ]
    }
   ],
   "source": [
    "!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89f7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import sklearn\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from acv_explainers import ACXplainer\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, rand, early_stop\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013f3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset = \"bpic2011\"\n",
    "bucket_method = \"prefix\"\n",
    "encoding = \"index\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "method_name = bucket_method+\"_\"+encoding\n",
    "\n",
    "random_state = 22\n",
    "exp_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd717205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mythreyi/full_stability/bpic2011/nb/prefix_index\n",
      "/home/mythreyi/full_stability/bpic2011/datasets\n"
     ]
    }
   ],
   "source": [
    "method_folder = os.path.join(PATH, dataset, cls_method, method_name)\n",
    "dataset_folder = os.path.join(PATH, dataset, \"datasets\")\n",
    "\n",
    "print(method_folder)\n",
    "print(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648967dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"],\n",
    "    \"bpic2011\" : [\"bpic2011_f1\"],\n",
    "    \"hospital\" : [\"hospital_billing_2\"],\n",
    "    \"traffic\" : [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset] if dataset not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset]\n",
    "\n",
    "num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset, cls_method, method_name)))])\n",
    "\n",
    "# for dataset_name in datasets:\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "    \n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = num_buckets\n",
    "\n",
    "#     dt_train_prefixes = pd.read_csv(os.path.join(dataset_folder, \"train_prefixes.csv\"))\n",
    "#     dt_train_prefixes = dataset_manager.generate_prefix_data(dt_train_prefixes, min_prefix_length, max_prefix_length)\n",
    "\n",
    "#     dt_val_prefixes = pd.read_csv(os.path.join(dataset_folder, \"val_prefixes.csv\"))\n",
    "#     dt_val_prefixes = dataset_manager.generate_prefix_data(dt_val_prefixes, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "#     dt_test_prefixes = pd.read_csv(os.path.join(dataset_folder, \"test_prefixes.csv\"))\n",
    "#     dt_test_prefixes = dataset_manager.generate_prefix_data(dt_test_prefixes, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "#     if bucket_method == \"state\":\n",
    "#         bucket_encoding = \"last\"\n",
    "#     else:\n",
    "#         bucket_encoding = \"agg\"\n",
    "    \n",
    "#     bucketer_args = {'encoding_method':bucket_encoding,\n",
    "#                      'case_id_col':dataset_manager.case_id_col, \n",
    "#                      'cat_cols':[dataset_manager.activity_col], \n",
    "#                      'num_cols':[], \n",
    "#                      'random_state':random_state}\n",
    "#     bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "\n",
    "#     bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "#     bucket_assignments_val = bucketer.predict(dt_val_prefixes)\n",
    "#     bucket_assignments_test = bucketer.predict(dt_test_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33a0c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cc1469b70a47388cc61399f7096cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:02,  1.27s/trial, best loss: -0.6827121599197364]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:18,  1.63s/trial, best loss: -0.7271385131622157]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:08,  1.45s/trial, best loss: -0.7271385131622157]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<01:03,  1.38s/trial, best loss: -0.7271385131622157]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:03,  1.42s/trial, best loss: -0.7271385131622157]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8012232415902141\n",
      "Testing Score: 0.6\n",
      "Bucket 2\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:04,  1.32s/trial, best loss: -0.7066421264250058]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:19,  1.66s/trial, best loss: -0.7415842772884179]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:07,  1.44s/trial, best loss: -0.7415842772884179]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<01:05,  1.43s/trial, best loss: -0.7415842772884179]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:04,  1.44s/trial, best loss: -0.7415842772884179]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.7999999999999999\n",
      "Testing Score: 0.25\n",
      "Bucket 3\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:03,  1.29s/trial, best loss: -0.7455486500464286]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:18,  1.64s/trial, best loss: -0.7504394751524849]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:08,  1.45s/trial, best loss: -0.7504394751524849]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<01:01,  1.33s/trial, best loss: -0.7504394751524849]\u001b[A\n",
      " 10%|▉        | 5/50 [00:06<01:01,  1.37s/trial, best loss: -0.7504394751524849]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8162083936324167\n",
      "Testing Score: 0.923076923076923\n",
      "Bucket 4\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:02,  1.27s/trial, best loss: -0.7526281171537532]\u001b[A\n",
      "  4%|▎        | 2/50 [00:02<01:11,  1.50s/trial, best loss: -0.7630968222177074]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:01,  1.30s/trial, best loss: -0.7630968222177074]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<00:59,  1.30s/trial, best loss: -0.7630968222177074]\u001b[A\n",
      " 10%|▉        | 5/50 [00:06<00:58,  1.31s/trial, best loss: -0.7630968222177074]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8249258160237388\n",
      "Testing Score: 0.8000000000000002\n",
      "Bucket 5\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<00:54,  1.11s/trial, best loss: -0.7407145432187914]\u001b[A\n",
      "  4%|▎        | 2/50 [00:02<01:10,  1.46s/trial, best loss: -0.7599483204134367]\u001b[A\n",
      "  6%|▌        | 3/50 [00:03<01:00,  1.28s/trial, best loss: -0.7613147550616208]\u001b[A\n",
      "  8%|▋        | 4/50 [00:04<00:53,  1.16s/trial, best loss: -0.7613147550616208]\u001b[A\n",
      " 10%|▉        | 5/50 [00:05<00:50,  1.11s/trial, best loss: -0.7613147550616208]\u001b[A\n",
      " 12%|█        | 6/50 [00:07<00:53,  1.22s/trial, best loss: -0.7613147550616208]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.752808988764045\n",
      "Testing Score: 0.9333333333333333\n",
      "Bucket 6\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<00:49,  1.01s/trial, best loss: -0.7497359006379508]\u001b[A\n",
      "  4%|▎        | 2/50 [00:02<00:58,  1.23s/trial, best loss: -0.7497359006379508]\u001b[A\n",
      "  6%|▌        | 3/50 [00:03<00:52,  1.13s/trial, best loss: -0.7497359006379508]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.7894736842105263\n",
      "Testing Score: 0.9090909090909091\n",
      "Bucket 7\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<00:56,  1.15s/trial, best loss: -0.7630959560857835]\u001b[A\n",
      "  4%|▎        | 2/50 [00:02<01:07,  1.40s/trial, best loss: -0.7672405735798593]\u001b[A\n",
      "  6%|▌        | 3/50 [00:03<00:58,  1.25s/trial, best loss: -0.7672405735798593]\u001b[A\n",
      "  8%|▋        | 4/50 [00:04<00:55,  1.20s/trial, best loss: -0.7672405735798593]\u001b[A\n",
      " 10%|▉        | 5/50 [00:06<00:55,  1.23s/trial, best loss: -0.7672405735798593]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8270676691729324\n",
      "Testing Score: 0.9333333333333333\n",
      "Bucket 8\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏         | 1/50 [00:01<01:00,  1.24s/trial, best loss: -0.772492325353465]\u001b[A\n",
      "  4%|▎        | 2/50 [00:02<01:10,  1.46s/trial, best loss: -0.7768411464946224]\u001b[A\n",
      "  6%|▌        | 3/50 [00:03<01:01,  1.31s/trial, best loss: -0.7768411464946224]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<00:58,  1.28s/trial, best loss: -0.7768411464946224]\u001b[A\n",
      " 10%|▉        | 5/50 [00:06<00:59,  1.32s/trial, best loss: -0.7768411464946224]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8831168831168831\n",
      "Testing Score: 0.8571428571428571\n",
      "Bucket 9\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:11,  1.46s/trial, best loss: -0.7700726033475556]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:16,  1.59s/trial, best loss: -0.7917787828052181]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:10,  1.51s/trial, best loss: -0.7917787828052181]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<01:04,  1.41s/trial, best loss: -0.7932002008013709]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:03,  1.40s/trial, best loss: -0.7932002008013709]\u001b[A\n",
      " 12%|█        | 6/50 [00:08<01:03,  1.44s/trial, best loss: -0.7979948731278401]\u001b[A\n",
      " 14%|█▎       | 7/50 [00:10<01:11,  1.66s/trial, best loss: -0.7979948731278401]\u001b[A\n",
      " 16%|█▍       | 8/50 [00:12<01:08,  1.62s/trial, best loss: -0.7979948731278401]\u001b[A\n",
      " 18%|█▌       | 9/50 [00:13<01:03,  1.55s/trial, best loss: -0.7979948731278401]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.942339373970346\n",
      "Testing Score: 0.8333333333333334\n",
      "Bucket 10\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:06,  1.37s/trial, best loss: -0.7555440515491189]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:17,  1.61s/trial, best loss: -0.7749419851014133]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:08,  1.46s/trial, best loss: -0.7749419851014133]\u001b[A\n",
      "  8%|▋        | 4/50 [00:05<01:05,  1.41s/trial, best loss: -0.7749419851014133]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:04,  1.43s/trial, best loss: -0.7749419851014133]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8762541806020068\n",
      "Testing Score: 0.8571428571428571\n",
      "Bucket 11\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:06,  1.35s/trial, best loss: -0.8077960101838728]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:16,  1.60s/trial, best loss: -0.8077960101838728]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:10,  1.49s/trial, best loss: -0.8077960101838728]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8031007751937985\n",
      "Testing Score: 1.0\n",
      "Bucket 12\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:16,  1.55s/trial, best loss: -0.7727021120671352]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:25,  1.79s/trial, best loss: -0.7888187545886133]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:15,  1.61s/trial, best loss: -0.7888187545886133]\u001b[A\n",
      "  8%|▋        | 4/50 [00:06<01:08,  1.49s/trial, best loss: -0.7888187545886133]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:09,  1.54s/trial, best loss: -0.7888187545886133]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8851351351351351\n",
      "Testing Score: 0.8333333333333333\n",
      "Bucket 13\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:22,  1.69s/trial, best loss: -0.7504847608334673]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:21,  1.70s/trial, best loss: -0.7760899093529622]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:14,  1.58s/trial, best loss: -0.7760899093529622]\u001b[A\n",
      "  8%|▋        | 4/50 [00:06<01:10,  1.53s/trial, best loss: -0.7760899093529622]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:09,  1.55s/trial, best loss: -0.7760899093529622]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8892794376098417\n",
      "Testing Score: 0.8571428571428571\n",
      "Bucket 14\n",
      "importing ddata and models...\n",
      "getting og model predictions...\n",
      "setting up hyperparameters...\n",
      "testing hyperparameters\n",
      "\n",
      "  0%|                                    | 0/50 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "  2%|▏        | 1/50 [00:01<01:17,  1.59s/trial, best loss: -0.7819870992918126]\u001b[A\n",
      "  4%|▎        | 2/50 [00:03<01:21,  1.69s/trial, best loss: -0.7926364554361465]\u001b[A\n",
      "  6%|▌        | 3/50 [00:04<01:16,  1.62s/trial, best loss: -0.7926364554361465]\u001b[A\n",
      "  8%|▋        | 4/50 [00:06<01:12,  1.57s/trial, best loss: -0.7926364554361465]\u001b[A\n",
      " 10%|▉        | 5/50 [00:07<01:10,  1.57s/trial, best loss: -0.7926364554361465]\u001b[A\n",
      "training surrogate model\n",
      "Training Score: 0.8836424957841484\n",
      "Testing Score: 0.6\n"
     ]
    }
   ],
   "source": [
    "for bucket in tqdm_notebook(range(num_buckets)):\n",
    "    bucketID = bucket+1\n",
    "    print ('Bucket', bucketID)\n",
    "\n",
    "    #import everything needed to sort and predict\n",
    "    print(\"importing ddata and models...\")\n",
    "    pipeline_path = os.path.join(method_folder, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                 (bucketID))\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    feature_combiner = pipeline['encoder']\n",
    "    if 'scaler' in pipeline.named_steps:\n",
    "        scaler = pipeline['scaler']\n",
    "    else:\n",
    "        scaler = None\n",
    "    cls = pipeline['cls']\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(method_folder, \"train_data\", \"train_data_bucket_%s.csv\" % (bucketID)))\n",
    "    if scaler!= None:\n",
    "        X_train = scaler.transform(X_train)\n",
    "    Y_train = pd.read_csv(os.path.join(method_folder, \"train_data\", \"y_train_bucket_%s.csv\" % (bucketID)))\n",
    "    \n",
    "    test_x = pd.read_csv(os.path.join(method_folder, \"samples\", \"test_sample_bucket_%s.csv\" % (bucketID)))\n",
    "    if scaler!=None:\n",
    "        test_x = scaler.transform(test_x)\n",
    "    #print(feature_combiner, scaler, cls)\n",
    "    \n",
    "#     if scaler!=None:\n",
    "#         X_train = scaler.transform(train_data)\n",
    "\n",
    "#     relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucketID]\n",
    "#     dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket)\n",
    "\n",
    "#     X_train = feature_combiner.transform(dt_train_bucket)\n",
    "#     if scaler!=None:\n",
    "#         X_train = scaler.transform(X_train)\n",
    "        \n",
    "#     relevant_val_cases_bucket = dataset_manager.get_indexes(dt_val_prefixes)[bucket_assignments_val == bucketID]\n",
    "#     dt_val_bucket = dataset_manager.get_relevant_data_by_indexes(dt_val_prefixes, relevant_val_cases_bucket)\n",
    "\n",
    "#     X_val = feature_combiner.transform(dt_val_bucket)\n",
    "#     if scaler!=None:\n",
    "#         X_val = scaler.transform(X_val)\n",
    "    \n",
    "#     relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucketID]\n",
    "#     dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "#     test_x = feature_combiner.transform(dt_test_bucket)\n",
    "#     if scaler!=None:\n",
    "#         test_x = scaler.transform(test_x)\n",
    "    \n",
    "    print(\"getting og model predictions...\")\n",
    "    Y_pred = cls.predict(X_train)\n",
    "    test_pred = cls.predict(test_x)\n",
    "    \n",
    "#     full_train_x = np.vstack((X_train, X_val))\n",
    "#     full_train_y = np.hstack((Y_pred, Y_val))\n",
    "    \n",
    "    #Set up hyperparameter optimisation\n",
    "    print(\"setting up hyperparameters...\")\n",
    "    kf = KFold(n_splits=5, shuffle = True, random_state=random_state)\n",
    "\n",
    "    space = {\"n_estimators\": scope.int(hp.quniform('n_estimators', 1, 20, q=1)),\n",
    "            \"max_depth\": scope.int(hp.quniform('max_depth', 1, 20, q=1)),\n",
    "            \"sample_fraction\": (hp.quniform('sample_fraction', 0.0001, 1, q=0.4))}\n",
    "\n",
    "    trials = Trials()\n",
    "    \n",
    "    def acv_classifier_optimisation(args, random_state = random_state, cv = kf, X = X_train, y = Y_pred):\n",
    "        score = []\n",
    "        iteration = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "#             iteration += 1\n",
    "#             print(\"Testing fold no.\", iteration)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            estimator = ACXplainer(classifier = True, n_estimators = args[\"n_estimators\"], \n",
    "                                   max_depth = args['max_depth'], sample_fraction = args[\"sample_fraction\"])\n",
    "            estimator.fit(X_train, y_train)\n",
    "\n",
    "            score.append(f1_score(y_test, estimator.predict(X_test)))\n",
    "        \n",
    "        score = np.mean(score)\n",
    "\n",
    "        return -score\n",
    "    \n",
    "    print(\"testing hyperparameters\")\n",
    "    best = fmin(acv_classifier_optimisation, verbose=1, space = space, algo=rand.suggest, max_evals = 50, trials=trials, \n",
    "                rstate=np.random.default_rng(random_state), early_stop_fn=early_stop.no_progress_loss(3))\n",
    "    print(\"training surrogate model\")\n",
    "    explainer = ACXplainer(classifier = True, n_estimators = int(best['n_estimators']), \n",
    "                           max_depth = int(best['max_depth']), sample_fraction = best['sample_fraction'])\n",
    "    explainer.fit(X_train, Y_pred)\n",
    "    \n",
    "    print(\"Training Score:\", f1_score(cls.predict(X_train), explainer.predict(X_train)))\n",
    "    print(\"Testing Score:\", f1_score(cls.predict(test_x), explainer.predict(test_x)))\n",
    "    \n",
    "    joblib.dump(explainer, method_folder+\"/acv_surrogate/acv_explainer_bucket_%s.joblib\"%(bucketID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ee7d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': 1,\n",
       " 'n_estimators': 18,\n",
       " 'verbose': False,\n",
       " 'mtry': 0,\n",
       " 'importance': 'impurity',\n",
       " 'min_node_size': 0,\n",
       " 'max_depth': 12,\n",
       " 'replace': True,\n",
       " 'sample_fraction': 0.4,\n",
       " 'keep_inbag': False,\n",
       " 'inbag': None,\n",
       " 'split_rule': 'gini',\n",
       " 'num_random_splits': 1,\n",
       " 'check_is_explain': False,\n",
       " 'ACXplainer': None,\n",
       " 'seed': 2021,\n",
       " 'rules': None,\n",
       " 'rules_output': None,\n",
       " 'rules_s_star': None,\n",
       " 'rules_coverage': None,\n",
       " 'rules_acc': None,\n",
       " 'rules_var': None,\n",
       " 'd': 2245,\n",
       " 'check_is_globalrule': False,\n",
       " 'rules_output_proba': None,\n",
       " 'rules_ori': None,\n",
       " 'rules_s_star_ori': None,\n",
       " 'model': RangerForestClassifier(enable_tree_details=True, importance='impurity',\n",
       "                        max_depth=12, n_estimators=18, sample_fraction=0.4,\n",
       "                        seed=2021)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbbee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = joblib.load(\"bpic2012/nb/single_agg/acv_surrogate/acv_explainer_bucket_1.joblib\")\n",
    "# vars(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5727e0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2245)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == 16]\n",
    "# dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "# test_x = feature_combiner.transform(dt_test_bucket)\n",
    "# if scaler!=None:\n",
    "#     test_x = scaler.transform(test_x)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebfa920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [0, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(explainer.predict(test_x), cls.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45d02152",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(explainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mfull_train_x\u001b[49m), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(full_train_x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_train_x' is not defined"
     ]
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(explainer.predict(full_train_x), cls.predict(full_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d034e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.hstack((dataset_manager.get_label_numeric(dt_train_bucket), dataset_manager.get_label_numeric(dt_val_bucket)))\n",
    "sklearn.metrics.confusion_matrix(true, cls.predict(full_train_x))\n",
    "f1_score(true, cls.predict(full_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ea19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(dataset_manager.get_label_numeric(dt_test_bucket), cls.predict(test_x))\n",
    "f1_score(dataset_manager.get_label_numeric(dt_test_bucket), cls.predict(test_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
