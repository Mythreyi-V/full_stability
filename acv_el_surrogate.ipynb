{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f21d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import sklearn\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from acv_explainers import ACXplainer\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, rand, early_stop\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eea3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset = \"bpic2012\"\n",
    "bucket_method = \"single\"\n",
    "encoding = \"agg\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "method_name = bucket_method+\"_\"+encoding\n",
    "\n",
    "random_state = 22\n",
    "exp_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9723cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mythreyi/full_stability/bpic2012/nb/single_agg\n",
      "/home/mythreyi/full_stability/bpic2012/datasets\n"
     ]
    }
   ],
   "source": [
    "method_folder = os.path.join(PATH, dataset, cls_method, method_name)\n",
    "dataset_folder = os.path.join(PATH, dataset, \"datasets\")\n",
    "\n",
    "print(method_folder)\n",
    "print(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce21beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"]\n",
    "    \"bpic2011\" : [\"bpic2011_f1\"]\n",
    "    \"hospital\" : [\"hospital_billing_2\"]\n",
    "    \"traffic\" : [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset] if dataset not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset]\n",
    "\n",
    "num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset, cls_method, method_name)))])\n",
    "\n",
    "# for dataset_name in datasets:\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "    \n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = num_buckets\n",
    "\n",
    "#     dt_train_prefixes = pd.read_csv(os.path.join(dataset_folder, \"train_prefixes.csv\"))\n",
    "#     dt_train_prefixes = dataset_manager.generate_prefix_data(dt_train_prefixes, min_prefix_length, max_prefix_length)\n",
    "\n",
    "#     dt_val_prefixes = pd.read_csv(os.path.join(dataset_folder, \"val_prefixes.csv\"))\n",
    "#     dt_val_prefixes = dataset_manager.generate_prefix_data(dt_val_prefixes, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "#     dt_test_prefixes = pd.read_csv(os.path.join(dataset_folder, \"test_prefixes.csv\"))\n",
    "#     dt_test_prefixes = dataset_manager.generate_prefix_data(dt_test_prefixes, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "#     if bucket_method == \"state\":\n",
    "#         bucket_encoding = \"last\"\n",
    "#     else:\n",
    "#         bucket_encoding = \"agg\"\n",
    "    \n",
    "#     bucketer_args = {'encoding_method':bucket_encoding,\n",
    "#                      'case_id_col':dataset_manager.case_id_col, \n",
    "#                      'cat_cols':[dataset_manager.activity_col], \n",
    "#                      'num_cols':[], \n",
    "#                      'random_state':random_state}\n",
    "#     bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "\n",
    "#     bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "#     bucket_assignments_val = bucketer.predict(dt_val_prefixes)\n",
    "#     bucket_assignments_test = bucketer.predict(dt_test_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b461e8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4a9f8d22194792a806a2e8fc0a1c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "Training Score: 0.9837659728361328\n",
      "Testing Score: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "for bucket in tqdm_notebook(range(num_buckets)):\n",
    "    bucketID = bucket+1\n",
    "    print ('Bucket', bucketID)\n",
    "\n",
    "    #import everything needed to sort and predict\n",
    "    pipeline_path = os.path.join(method_folder, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                 (bucketID))\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    feature_combiner = pipeline['encoder']\n",
    "    if 'scaler' in pipeline.named_steps:\n",
    "        scaler = pipeline['scaler']\n",
    "    else:\n",
    "        scaler = None\n",
    "    cls = pipeline['cls']\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(method_folder, \"train_data\", \"train_data_bucket_%s.csv\" % (bucketID)))\n",
    "    if scaler!= None:\n",
    "        X_train = scaler.transform(X_train)\n",
    "    Y_train = pd.read_csv(os.path.join(method_folder, \"train_data\", \"y_train_bucket_%s.csv\" % (bucketID)))\n",
    "    \n",
    "    test_x = pd.read_csv(os.path.join(method_folder, \"samples\", \"test_sample_bucket_%s.csv\" % (bucketID)))\n",
    "    if scaler!=None:\n",
    "        test_x = scaler.transform(test_x)\n",
    "    #print(feature_combiner, scaler, cls)\n",
    "    \n",
    "#     if scaler!=None:\n",
    "#         X_train = scaler.transform(train_data)\n",
    "\n",
    "#     relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucketID]\n",
    "#     dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket)\n",
    "\n",
    "#     X_train = feature_combiner.transform(dt_train_bucket)\n",
    "#     if scaler!=None:\n",
    "#         X_train = scaler.transform(X_train)\n",
    "        \n",
    "#     relevant_val_cases_bucket = dataset_manager.get_indexes(dt_val_prefixes)[bucket_assignments_val == bucketID]\n",
    "#     dt_val_bucket = dataset_manager.get_relevant_data_by_indexes(dt_val_prefixes, relevant_val_cases_bucket)\n",
    "\n",
    "#     X_val = feature_combiner.transform(dt_val_bucket)\n",
    "#     if scaler!=None:\n",
    "#         X_val = scaler.transform(X_val)\n",
    "    \n",
    "#     relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucketID]\n",
    "#     dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "#     test_x = feature_combiner.transform(dt_test_bucket)\n",
    "#     if scaler!=None:\n",
    "#         test_x = scaler.transform(test_x)\n",
    "    \n",
    "    \n",
    "    Y_pred = cls.predict(X_train)\n",
    "    test_pred = cls.predict(test_x)\n",
    "    \n",
    "#     full_train_x = np.vstack((X_train, X_val))\n",
    "#     full_train_y = np.hstack((Y_pred, Y_val))\n",
    "    \n",
    "    #Set up hyperparameter optimisation\n",
    "    kf = KFold(n_splits=5, shuffle = True, random_state=random_state)\n",
    "\n",
    "    space = {\"n_estimators\": scope.int(hp.quniform('n_estimators', 1, 20, q=1)),\n",
    "            \"max_depth\": scope.int(hp.quniform('max_depth', 1, 20, q=1)),\n",
    "            \"sample_fraction\": (hp.quniform('sample_fraction', 0.0001, 1, q=0.4))}\n",
    "\n",
    "    trials = Trials()\n",
    "    \n",
    "    def acv_classifier_optimisation(args, random_state = random_state, cv = kf, X = X_train, y = Y_pred):\n",
    "        score = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            estimator = ACXplainer(classifier = True, n_estimators = args[\"n_estimators\"], \n",
    "                                   max_depth = args['max_depth'], sample_fraction = args[\"sample_fraction\"])\n",
    "            estimator.fit(X_train, y_train)\n",
    "\n",
    "            score.append(f1_score(y_test, estimator.predict(X_test)))\n",
    "        \n",
    "        score = np.mean(score)\n",
    "\n",
    "        return -score\n",
    "\n",
    "    best = fmin(acv_classifier_optimisation, verbose=0, space = space, algo=rand.suggest, max_evals = 50, trials=trials, \n",
    "                rstate=np.random.default_rng(random_state), early_stop_fn=early_stop.no_progress_loss(3))\n",
    "    explainer = ACXplainer(classifier = True, n_estimators = int(best['n_estimators']), \n",
    "                           max_depth = int(best['max_depth']), sample_fraction = best['sample_fraction'])\n",
    "    explainer.fit(X_train, Y_pred)\n",
    "    \n",
    "    print(\"Training Score:\", f1_score(cls.predict(X_train), explainer.predict(X_train)))\n",
    "    print(\"Testing Score:\", f1_score(cls.predict(test_x), explainer.predict(test_x)))\n",
    "    \n",
    "    joblib.dump(explainer, method_folder+\"/acv_surrogate/acv_explainer_bucket_%s.joblib\"%(bucketID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e91a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': 1,\n",
       " 'n_estimators': 18,\n",
       " 'verbose': False,\n",
       " 'mtry': 0,\n",
       " 'importance': 'impurity',\n",
       " 'min_node_size': 0,\n",
       " 'max_depth': 12,\n",
       " 'replace': True,\n",
       " 'sample_fraction': 0.4,\n",
       " 'keep_inbag': False,\n",
       " 'inbag': None,\n",
       " 'split_rule': 'gini',\n",
       " 'num_random_splits': 1,\n",
       " 'check_is_explain': False,\n",
       " 'ACXplainer': None,\n",
       " 'seed': 2021,\n",
       " 'rules': None,\n",
       " 'rules_output': None,\n",
       " 'rules_s_star': None,\n",
       " 'rules_coverage': None,\n",
       " 'rules_acc': None,\n",
       " 'rules_var': None,\n",
       " 'd': 120,\n",
       " 'check_is_globalrule': False,\n",
       " 'rules_output_proba': None,\n",
       " 'rules_ori': None,\n",
       " 'rules_s_star_ori': None,\n",
       " 'model': RangerForestClassifier(enable_tree_details=True, importance='impurity',\n",
       "                        max_depth=12, n_estimators=18, sample_fraction=0.4,\n",
       "                        seed=2021)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429a4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = joblib.load(\"bpic2012/nb/single_agg/acv_surrogate/acv_explainer_bucket_1.joblib\")\n",
    "# vars(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9be60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == 16]\n",
    "# dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "# test_x = feature_combiner.transform(dt_test_bucket)\n",
    "# if scaler!=None:\n",
    "#     test_x = scaler.transform(test_x)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "471d0c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[89,  1],\n",
       "       [ 0, 22]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(explainer.predict(test_x), cls.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298e1386",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mconfusion_matrix(explainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mfull_train_x\u001b[49m), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(full_train_x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_train_x' is not defined"
     ]
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(explainer.predict(full_train_x), cls.predict(full_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.hstack((dataset_manager.get_label_numeric(dt_train_bucket), dataset_manager.get_label_numeric(dt_val_bucket)))\n",
    "sklearn.metrics.confusion_matrix(true, cls.predict(full_train_x))\n",
    "f1_score(true, cls.predict(full_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99564a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(dataset_manager.get_label_numeric(dt_test_bucket), cls.predict(test_x))\n",
    "f1_score(dataset_manager.get_label_numeric(dt_test_bucket), cls.predict(test_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
