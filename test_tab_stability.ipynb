{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mythreyi/.local/lib/python3.9/site-packages/xgboost/compat.py:85: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-01-27 11:41:21.110812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 11:41:21.110874: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "#import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "#from DatasetManager import DatasetManager\n",
    "#import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from acv_explainers import ACXplainer\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def imp_df(column_names, importances):\n",
    "#         df = pd.DataFrame({'feature': column_names,\n",
    "#                        'feature_importance': importances}) \\\n",
    "#            .sort_values('feature_importance', ascending = False) \\\n",
    "#            .reset_index(drop = True)\n",
    "#         return df\n",
    "\n",
    "# # plotting a feature importance dataframe (horizontal barchart)\n",
    "# def var_imp_plot(imp_df, title, num_feat):\n",
    "#         imp_df.columns = ['feature', 'feature_importance']\n",
    "#         b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))[0]\n",
    "            #print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[i]\n",
    "            abs_val = abs(shap_values[i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] > q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter):\n",
    "    instance = instance.reshape(1, -1)\n",
    "    y = cls.predict(instance)\n",
    "    \n",
    "    t=np.var(y_train)\n",
    "\n",
    "    feats = []\n",
    "    feat_imp = []\n",
    "\n",
    "    for i in range(exp_iter):\n",
    "        sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train,\n",
    "                                                                                 t=t, pi_level=0.8)\n",
    "        clean_expl = sufficient_expl.copy()\n",
    "        clean_expl = clean_expl[0]\n",
    "        clean_expl = [sublist for sublist in clean_expl if sum(n<0 for n in sublist)==0 ]\n",
    "\n",
    "        clean_sdp = sdp_expl[0].copy()\n",
    "        clean_sdp = [sdp for sdp in clean_sdp if sdp > 0]\n",
    "        \n",
    "        lximp = explainer.compute_local_sdp(X_train.shape[1], clean_expl)\n",
    "        feat_imp.append(lximp)\n",
    "        \n",
    "        if len(clean_expl)==0 or len(clean_expl[0])==0:\n",
    "            print(\"No explamation meets pi level\")\n",
    "        else:\n",
    "            lens = [len(i) for i in clean_expl]\n",
    "            me_loc = [i for i in range(len(lens)) if lens[i]==min(lens)]\n",
    "            mse_loc = np.argmax(np.array(clean_sdp)[me_loc])\n",
    "            mse = np.array(clean_expl)[me_loc][mse_loc]\n",
    "            feats.extend(mse)\n",
    "\n",
    "    if len(feats)==0:\n",
    "        feat_pos = []\n",
    "    else:\n",
    "        feat_pos = set(feats)\n",
    "    \n",
    "      \n",
    "    feat_imp = np.mean(feat_imp, axis=0)\n",
    "    \n",
    "    return feat_imp, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [],
   "source": [
    "dataset_ref = \"nursery\"\n",
    "cls_method = \"logit\"\n",
    "\n",
    "xai_method = \"ACV\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'continuous': None,\n",
       " 'discrete': ['parents_great_pret',\n",
       "  'parents_pretentious',\n",
       "  'parents_usual',\n",
       "  'has_nurs_critical',\n",
       "  'has_nurs_improper',\n",
       "  'has_nurs_less_proper',\n",
       "  'has_nurs_proper',\n",
       "  'has_nurs_very_crit',\n",
       "  'form_complete',\n",
       "  'form_completed',\n",
       "  'form_foster',\n",
       "  'form_incomplete',\n",
       "  'children_1',\n",
       "  'children_2',\n",
       "  'children_3',\n",
       "  'children_more',\n",
       "  'housing_convenient',\n",
       "  'housing_critical',\n",
       "  'housing_less_conv',\n",
       "  'finance_convenient',\n",
       "  'finance_inconv',\n",
       "  'social_nonprob',\n",
       "  'social_problematic',\n",
       "  'social_slightly_prob',\n",
       "  'health_not_recom',\n",
       "  'health_priority',\n",
       "  'health_recommended']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(PATH, dataset_ref)\n",
    "\n",
    "cls = joblib.load(os.path.join(dataset_path, cls_method, \"cls.joblib\"))\n",
    "scaler = joblib.load(os.path.join(dataset_path, \"scaler.joblib\"))\n",
    "\n",
    "trainingdata = pd.read_csv(os.path.join(dataset_path, \"datasets\", dataset_ref+\"_Xtrain.csv\"), sep=\";\")\n",
    "y_train = pd.read_csv(os.path.join(dataset_path, \"datasets\", dataset_ref+\"_Ytrain.csv\"), sep=\";\")\n",
    "\n",
    "with open(os.path.join(dataset_path, \"datasets\",'col_dict.json')) as file:\n",
    "    col_dict = json.load(file)\n",
    "file.close()\n",
    "\n",
    "sample_instances = pd.read_csv((os.path.join(dataset_path, cls_method, \"test_sample.csv\")), sep=\";\") \n",
    "results = pd.read_csv((os.path.join(dataset_path, cls_method, \"results.csv\")))#, sep=\";\") \n",
    "targets = results[\"Actual\"]\n",
    "col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"SHAP\":\n",
    "\n",
    "    if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "        shap_explainer = shap.Explainer(cls)\n",
    "    elif cls_method == \"nb\":\n",
    "        shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "    else:\n",
    "        shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "    print(type(shap_explainer))\n",
    "\n",
    "    feat_list = trainingdata.columns.tolist()\n",
    "\n",
    "    subset_stability = []\n",
    "    weight_stability = []\n",
    "    adjusted_weight_stability = []\n",
    "\n",
    "    #explain the chosen instances and find the stability score\n",
    "    instance_no = 0\n",
    "    for instance in tqdm_notebook(sample_instances.values):\n",
    "        instance_no += 1    \n",
    "        print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "        #if cls_method == \"xgboost\":\n",
    "        instance = instance.reshape(1, -1)\n",
    "        pred = cls.predict(instance)\n",
    "\n",
    "        #Get Tree SHAP explanations for instance\n",
    "        exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "        feat_pres = []\n",
    "        feat_weights = []\n",
    "\n",
    "        for iteration in rel_exp:\n",
    "            #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "            presence_list = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "\n",
    "                for explanation in iteration:\n",
    "                    if each in explanation[0]:\n",
    "                        presence_list[list_idx] = 1\n",
    "\n",
    "            feat_pres.append(presence_list)\n",
    "\n",
    "        for iteration in exp:\n",
    "            #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "            weights = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "\n",
    "                for explanation in iteration:\n",
    "                    if each in explanation[0]:\n",
    "\n",
    "                        weights[list_idx] = explanation[1]\n",
    "            feat_weights.append(weights)\n",
    "\n",
    "        stability = st.getStability(feat_pres)\n",
    "        print (\"Stability:\", round(stability,2))\n",
    "        subset_stability.append(stability)\n",
    "\n",
    "        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "        avg_dispersal = 1-np.mean(rel_var)\n",
    "        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "        weight_stability.append(avg_dispersal)\n",
    "        adj_dispersal = 1-np.mean(second_var)\n",
    "        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "        adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    results[\"SHAP Subset Stability\"] = subset_stability\n",
    "    results[\"SHAP Weight Stability\"] = weight_stability\n",
    "    results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction Probability</th>\n",
       "      <th>SHAP Subset Stability</th>\n",
       "      <th>SHAP Weight Stability</th>\n",
       "      <th>SHAP Adjusted Weight Stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Actual  Prediction  Prediction Probability  \\\n",
       "0            0       0           0                0.997319   \n",
       "1            1       0           0                0.997458   \n",
       "2            2       0           0                0.997196   \n",
       "3            3       0           0                0.997522   \n",
       "4            4       0           0                0.997529   \n",
       "..         ...     ...         ...                     ...   \n",
       "95          95       1           1                0.997351   \n",
       "96          96       1           1                0.997485   \n",
       "97          97       1           1                0.997405   \n",
       "98          98       1           1                0.997315   \n",
       "99          99       1           1                0.997434   \n",
       "\n",
       "    SHAP Subset Stability  SHAP Weight Stability  \\\n",
       "0                     1.0                    1.0   \n",
       "1                     1.0                    1.0   \n",
       "2                     1.0                    1.0   \n",
       "3                     1.0                    1.0   \n",
       "4                     1.0                    1.0   \n",
       "..                    ...                    ...   \n",
       "95                    1.0                    1.0   \n",
       "96                    1.0                    1.0   \n",
       "97                    1.0                    1.0   \n",
       "98                    1.0                    1.0   \n",
       "99                    1.0                    1.0   \n",
       "\n",
       "    SHAP Adjusted Weight Stability  \n",
       "0                              1.0  \n",
       "1                              1.0  \n",
       "2                              1.0  \n",
       "3                              1.0  \n",
       "4                              1.0  \n",
       "..                             ...  \n",
       "95                             1.0  \n",
       "96                             1.0  \n",
       "97                             1.0  \n",
       "98                             1.0  \n",
       "99                             1.0  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv(os.path.join(dataset_path, cls_method, \"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LIME\":\n",
    "    \n",
    "    feat_list = trainingdata.columns.tolist()\n",
    "    class_names = [\"Negative\", \"Positive\"]\n",
    "\n",
    "    cats = col_dict[\"discrete\"]\n",
    "\n",
    "    subset_stability = []\n",
    "    weight_stability = []\n",
    "    adjusted_weight_stability = []\n",
    "\n",
    "    #create explainer now that can be passed later\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata.values,\n",
    "                          feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "\n",
    "    instance_no = 0\n",
    "    print(len(sample_instances))\n",
    "    #explain the chosen instances and find the stability score\n",
    "    for instance in tqdm_notebook(sample_instances.values):\n",
    "        instance_no += 1\n",
    "\n",
    "        print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "        #Get lime explanations for instance\n",
    "        feat_pres = []\n",
    "        feat_weights = []\n",
    "\n",
    "        for iteration in list(range(exp_iter)):\n",
    "\n",
    "            lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                                  max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "            all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "            bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "\n",
    "            presence_list = [0]*len(feat_list)\n",
    "            weights = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "                #print (\"Feature\", list_idx)\n",
    "                for explanation in lime_exp.as_list():\n",
    "                    if each in explanation[0]:\n",
    "                        if explanation[1] > q1_min:\n",
    "                            presence_list[list_idx] = 1\n",
    "                        weights[list_idx] = explanation[1]\n",
    "\n",
    "            feat_pres.append(presence_list)\n",
    "            feat_weights.append(weights)\n",
    "\n",
    "        stability = st.getStability(feat_pres)\n",
    "        print (\"Stability:\", round(stability,2))\n",
    "        subset_stability.append(stability)\n",
    "\n",
    "        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "        avg_dispersal = 1-np.mean(rel_var)\n",
    "        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "        weight_stability.append(avg_dispersal)\n",
    "        adj_dispersal = 1-np.mean(second_var)\n",
    "        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "        adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    results[\"LIME Subset Stability\"] = subset_stability\n",
    "    results[\"LIME Weight Stability\"] = weight_stability\n",
    "    results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e469065a57f42479d76a42ee96a5d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 100 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████| 29/29 [00:00<00:00, 170.63it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▍                                       | 1/10 [00:01<00:17,  1.97s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:08<00:36,  4.58s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:33<01:38, 14.06s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [01:25<02:52, 28.83s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [02:00<02:36, 31.30s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [02:31<02:03, 31.00s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:49<01:20, 26.72s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [02:56<00:41, 20.57s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:58<00:14, 14.78s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:59<00:00, 17.90s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explamation meets pi level\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████▍                                       | 1/10 [00:02<00:18,  2.01s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:08<00:36,  4.62s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:24<01:09, 10.00s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [00:53<01:43, 17.23s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:28<01:59, 23.84s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:59<01:44, 26.03s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:16<01:09, 23.18s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [02:23<00:36, 18.01s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:25<00:13, 13.05s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:25<00:00, 14.59s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explamation meets pi level\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████▍                                       | 1/10 [00:01<00:17,  1.95s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:08<00:37,  4.65s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:24<01:10, 10.01s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [00:52<01:42, 17.12s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:27<01:57, 23.53s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:56<01:41, 25.36s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:13<01:07, 22.62s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [02:20<00:35, 17.60s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:22<00:12, 12.72s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:22<00:00, 14.29s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explamation meets pi level\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████▍                                       | 1/10 [00:01<00:16,  1.86s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:08<00:35,  4.39s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:23<01:07,  9.59s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [00:52<01:41, 16.94s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:26<01:56, 23.26s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [01:56<01:42, 25.62s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:14<01:09, 23.06s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [02:21<00:36, 18.02s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:23<00:13, 13.04s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:24<00:00, 14.42s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explamation meets pi level\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|████▍                                       | 1/10 [00:02<00:18,  2.01s/it]\u001b[A\n",
      " 20%|████████▊                                   | 2/10 [00:08<00:37,  4.67s/it]\u001b[A\n",
      " 30%|█████████████▏                              | 3/10 [00:25<01:11, 10.22s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [00:55<01:47, 17.98s/it]\u001b[A\n",
      " 50%|██████████████████████                      | 5/10 [01:30<02:01, 24.23s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [02:00<01:44, 26.06s/it]\u001b[A\n",
      " 70%|██████████████████████████████▊             | 7/10 [02:17<01:09, 23.24s/it]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [02:24<00:36, 18.05s/it]\u001b[A\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [02:26<00:12, 12.99s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [02:26<00:00, 14.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explamation meets pi level\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[]\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n"
     ]
    }
   ],
   "source": [
    "if xai_method==\"ACV\":\n",
    "            \n",
    "    acv_explainer = joblib.load(os.path.join(PATH, dataset_ref, cls_method,'acv_explainer.joblib'))\n",
    "\n",
    "    feat_list = trainingdata.columns.tolist()\n",
    "    \n",
    "    subset_stability = []\n",
    "    weight_stability = []\n",
    "    adjusted_weight_stability = []\n",
    "\n",
    "\n",
    "    instance_no = 0\n",
    "    print(len(sample_instances))\n",
    "    #explain the chosen instances and find the stability score\n",
    "    for instance in tqdm_notebook(sample_instances.values[:1]):\n",
    "        instance_no += 1\n",
    "\n",
    "        print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "        #Get acv explanations for instance\n",
    "        feat_pres = []\n",
    "        feat_weights = []\n",
    "\n",
    "        for iteration in list(range(exp_iter)):\n",
    "            weights, feat_pos = get_acv_features(acv_explainer, instance, cls, trainingdata, y_train, 1)\n",
    "            print(weights)\n",
    "            print(feat_pos)\n",
    "\n",
    "            presence_list = np.array([0]*len(feat_list))                    \n",
    "            presence_list[feat_pos] = 1\n",
    "\n",
    "            feat_pres.append(presence_list)\n",
    "            feat_weights.append(weights)\n",
    "\n",
    "        stability = st.getStability(feat_pres)\n",
    "        print (\"Stability:\", round(stability,2))\n",
    "        subset_stability.append(stability)\n",
    "\n",
    "        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "        avg_dispersal = 1-np.mean(rel_var)\n",
    "        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "        weight_stability.append(avg_dispersal)\n",
    "        adj_dispersal = 1-np.mean(second_var)\n",
    "        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "        adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    #             results[\"ACV Subset Stability\"] = subset_stability\n",
    "    #             results[\"ACV Weight Stability\"] = weight_stability\n",
    "    #             results[\"ACV Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "    #             all_results[bucket] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parents_great_pret</th>\n",
       "      <th>parents_pretentious</th>\n",
       "      <th>parents_usual</th>\n",
       "      <th>has_nurs_critical</th>\n",
       "      <th>has_nurs_improper</th>\n",
       "      <th>has_nurs_less_proper</th>\n",
       "      <th>has_nurs_proper</th>\n",
       "      <th>has_nurs_very_crit</th>\n",
       "      <th>form_complete</th>\n",
       "      <th>form_completed</th>\n",
       "      <th>...</th>\n",
       "      <th>housing_critical</th>\n",
       "      <th>housing_less_conv</th>\n",
       "      <th>finance_convenient</th>\n",
       "      <th>finance_inconv</th>\n",
       "      <th>social_nonprob</th>\n",
       "      <th>social_problematic</th>\n",
       "      <th>social_slightly_prob</th>\n",
       "      <th>health_not_recom</th>\n",
       "      <th>health_priority</th>\n",
       "      <th>health_recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6048 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      parents_great_pret  parents_pretentious  parents_usual  \\\n",
       "0                    0.0                  0.0            1.0   \n",
       "1                    1.0                  0.0            0.0   \n",
       "2                    1.0                  0.0            0.0   \n",
       "3                    0.0                  1.0            0.0   \n",
       "4                    0.0                  0.0            1.0   \n",
       "...                  ...                  ...            ...   \n",
       "6043                 1.0                  0.0            0.0   \n",
       "6044                 0.0                  0.0            1.0   \n",
       "6045                 0.0                  1.0            0.0   \n",
       "6046                 0.0                  0.0            1.0   \n",
       "6047                 1.0                  0.0            0.0   \n",
       "\n",
       "      has_nurs_critical  has_nurs_improper  has_nurs_less_proper  \\\n",
       "0                   0.0                0.0                   0.0   \n",
       "1                   0.0                0.0                   0.0   \n",
       "2                   0.0                0.0                   0.0   \n",
       "3                   0.0                0.0                   1.0   \n",
       "4                   0.0                0.0                   0.0   \n",
       "...                 ...                ...                   ...   \n",
       "6043                0.0                1.0                   0.0   \n",
       "6044                0.0                1.0                   0.0   \n",
       "6045                1.0                0.0                   0.0   \n",
       "6046                0.0                1.0                   0.0   \n",
       "6047                0.0                0.0                   0.0   \n",
       "\n",
       "      has_nurs_proper  has_nurs_very_crit  form_complete  form_completed  ...  \\\n",
       "0                 0.0                 1.0            0.0             1.0  ...   \n",
       "1                 0.0                 1.0            1.0             0.0  ...   \n",
       "2                 1.0                 0.0            0.0             1.0  ...   \n",
       "3                 0.0                 0.0            0.0             0.0  ...   \n",
       "4                 1.0                 0.0            1.0             0.0  ...   \n",
       "...               ...                 ...            ...             ...  ...   \n",
       "6043              0.0                 0.0            0.0             0.0  ...   \n",
       "6044              0.0                 0.0            0.0             0.0  ...   \n",
       "6045              0.0                 0.0            0.0             0.0  ...   \n",
       "6046              0.0                 0.0            0.0             0.0  ...   \n",
       "6047              0.0                 1.0            0.0             0.0  ...   \n",
       "\n",
       "      housing_critical  housing_less_conv  finance_convenient  finance_inconv  \\\n",
       "0                  0.0                0.0                 0.0             1.0   \n",
       "1                  1.0                0.0                 1.0             0.0   \n",
       "2                  0.0                0.0                 0.0             1.0   \n",
       "3                  0.0                0.0                 0.0             1.0   \n",
       "4                  0.0                0.0                 0.0             1.0   \n",
       "...                ...                ...                 ...             ...   \n",
       "6043               0.0                1.0                 0.0             1.0   \n",
       "6044               0.0                1.0                 1.0             0.0   \n",
       "6045               1.0                0.0                 0.0             1.0   \n",
       "6046               0.0                0.0                 1.0             0.0   \n",
       "6047               0.0                0.0                 0.0             1.0   \n",
       "\n",
       "      social_nonprob  social_problematic  social_slightly_prob  \\\n",
       "0                0.0                 1.0                   0.0   \n",
       "1                1.0                 0.0                   0.0   \n",
       "2                1.0                 0.0                   0.0   \n",
       "3                0.0                 0.0                   1.0   \n",
       "4                1.0                 0.0                   0.0   \n",
       "...              ...                 ...                   ...   \n",
       "6043             0.0                 1.0                   0.0   \n",
       "6044             1.0                 0.0                   0.0   \n",
       "6045             1.0                 0.0                   0.0   \n",
       "6046             0.0                 1.0                   0.0   \n",
       "6047             1.0                 0.0                   0.0   \n",
       "\n",
       "      health_not_recom  health_priority  health_recommended  \n",
       "0                  0.0              0.0                 1.0  \n",
       "1                  1.0              0.0                 0.0  \n",
       "2                  0.0              0.0                 1.0  \n",
       "3                  0.0              0.0                 1.0  \n",
       "4                  1.0              0.0                 0.0  \n",
       "...                ...              ...                 ...  \n",
       "6043               0.0              0.0                 1.0  \n",
       "6044               1.0              0.0                 0.0  \n",
       "6045               0.0              0.0                 1.0  \n",
       "6046               0.0              1.0                 0.0  \n",
       "6047               0.0              0.0                 1.0  \n",
       "\n",
       "[6048 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "95    1\n",
       "96    1\n",
       "97    1\n",
       "98    1\n",
       "99    1\n",
       "Name: Actual, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv(os.path.join(dataset_path, cls_method, \"results.csv\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
