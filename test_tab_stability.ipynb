{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "#from DatasetManager import DatasetManager\n",
    "#import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def imp_df(column_names, importances):\n",
    "#         df = pd.DataFrame({'feature': column_names,\n",
    "#                        'feature_importance': importances}) \\\n",
    "#            .sort_values('feature_importance', ascending = False) \\\n",
    "#            .reset_index(drop = True)\n",
    "#         return df\n",
    "\n",
    "# # plotting a feature importance dataframe (horizontal barchart)\n",
    "# def var_imp_plot(imp_df, title, num_feat):\n",
    "#         imp_df.columns = ['feature', 'feature_importance']\n",
    "#         b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))[0]\n",
    "            #print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[i]\n",
    "            abs_val = abs(shap_values[i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] > q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [],
   "source": [
    "dataset_ref = \"nursery\"\n",
    "cls_method = \"logit\"\n",
    "\n",
    "xai_method = \"SHAP\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'continuous': None,\n",
       " 'discrete': ['parents_great_pret',\n",
       "  'parents_pretentious',\n",
       "  'parents_usual',\n",
       "  'has_nurs_critical',\n",
       "  'has_nurs_improper',\n",
       "  'has_nurs_less_proper',\n",
       "  'has_nurs_proper',\n",
       "  'has_nurs_very_crit',\n",
       "  'form_complete',\n",
       "  'form_completed',\n",
       "  'form_foster',\n",
       "  'form_incomplete',\n",
       "  'children_1',\n",
       "  'children_2',\n",
       "  'children_3',\n",
       "  'children_more',\n",
       "  'housing_convenient',\n",
       "  'housing_critical',\n",
       "  'housing_less_conv',\n",
       "  'finance_convenient',\n",
       "  'finance_inconv',\n",
       "  'social_nonprob',\n",
       "  'social_problematic',\n",
       "  'social_slightly_prob',\n",
       "  'health_not_recom',\n",
       "  'health_priority',\n",
       "  'health_recommended']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dataset_name in datasets:\n",
    "\n",
    "#     min_prefix_length = 1\n",
    "\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "#     data = dataset_manager.read_dataset()\n",
    "\n",
    "#     all_pipelines = []\n",
    "#     all_cls = []\n",
    "#     all_encoders = []\n",
    "#     all_scalers = []\n",
    "#     all_train = []\n",
    "#     all_samples = []\n",
    "#     all_results = []\n",
    "    \n",
    "#     for ii in range(n_iter):\n",
    "#         num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "#         for bucket in tqdm_notebook(range(num_buckets)):\n",
    "#             bucketID = bucket+1\n",
    "#             print ('Bucket', bucketID)\n",
    "\n",
    "#             #import everything needed to sort and predict\n",
    "#             pipeline_path = os.path.join(PATH, \"%s/%s/%s/pipelines/pipeline_bucket_%s.joblib\" % \n",
    "#                                          (dataset_ref, cls_method, method_name, bucketID))\n",
    "#             pipeline = joblib.load(pipeline_path)\n",
    "#             feature_combiner = pipeline['encoder']\n",
    "#             if 'scaler' in pipeline.named_steps:\n",
    "#                 scaler = pipeline['scaler']\n",
    "#             else:\n",
    "#                 scaler = None\n",
    "#             cls = pipeline['cls']\n",
    "            \n",
    "#             all_cls.append(cls)\n",
    "#             all_encoders.append(feature_combiner)\n",
    "#             all_scalers.append(scaler)\n",
    "#             all_pipelines.append(pipeline)\n",
    "\n",
    "#             #find relevant samples for bucket\n",
    "#             bucket_sample = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             results_template = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID)))\n",
    "    \n",
    "#             if scaler != None:\n",
    "#                 bucket_sample = scaler.transform(bucket_sample)\n",
    "#             bucket_results = results_template\n",
    "            \n",
    "#             feat_names = feature_combiner.get_feature_names()\n",
    "#             feat_list = [feat.replace(\" \", \"_\") for feat in feat_names]\n",
    "            \n",
    "#             all_samples.append(bucket_sample)\n",
    "#             all_results.append(bucket_results)\n",
    "            \n",
    "#             #import training data for bucket\n",
    "#             train_data = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "#                                                           (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             if scaler != None:\n",
    "#                 train_data = scaler.transform(train_data)\n",
    "            \n",
    "#             all_train.append(train_data)\n",
    "\n",
    "dataset_path = os.path.join(PATH, dataset_ref)\n",
    "\n",
    "cls = joblib.load(os.path.join(dataset_path, cls_method, \"cls.joblib\"))\n",
    "scaler = joblib.load(os.path.join(dataset_path, \"scaler.joblib\"))\n",
    "\n",
    "trainingdata = pd.read_csv(os.path.join(dataset_path, \"datasets\", dataset_ref+\"_Xtrain.csv\"), sep=\";\")\n",
    "with open(os.path.join(dataset_path, \"datasets\",'col_dict.json')) as file:\n",
    "    col_dict = json.load(file)\n",
    "file.close()\n",
    "\n",
    "sample_instances = pd.read_csv((os.path.join(dataset_path, cls_method, \"test_sample.csv\")), sep=\";\") \n",
    "results = pd.read_csv((os.path.join(dataset_path, cls_method, \"results.csv\")), sep=\";\") \n",
    "col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'shap.explainers._linear.Linear'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfaf5a406a94911a409d83af8998723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 9 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 10 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 11 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 12 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 13 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 14 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 15 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 16 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 17 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 18 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 19 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 20 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 21 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 22 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 23 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 24 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 25 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 26 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 27 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 28 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 29 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 30 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 31 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 32 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 33 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 34 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 35 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 36 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 37 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 38 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 39 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 40 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 41 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 42 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 43 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 44 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 45 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 46 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 47 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 48 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 49 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 50 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 51 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 52 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 53 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 54 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 55 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 56 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 57 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 58 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 59 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 60 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 61 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 62 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 63 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 64 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 65 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 66 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 67 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 68 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 69 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 70 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 71 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 72 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 73 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 74 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 75 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 76 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 77 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 78 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 79 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 80 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 81 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 82 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 83 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 84 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 85 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 86 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 87 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 88 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 89 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 90 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 91 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 92 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 93 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 94 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 95 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 96 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 97 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 98 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 99 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 100 of 100 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if xai_method==\"SHAP\":\n",
    "\n",
    "    if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "        shap_explainer = shap.Explainer(cls)\n",
    "    elif cls_method == \"nb\":\n",
    "        shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "    else:\n",
    "        shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "    print(type(shap_explainer))\n",
    "\n",
    "    feat_list = trainingdata.columns.tolist()\n",
    "\n",
    "    subset_stability = []\n",
    "    weight_stability = []\n",
    "    adjusted_weight_stability = []\n",
    "\n",
    "    #explain the chosen instances and find the stability score\n",
    "    instance_no = 0\n",
    "    for instance in tqdm_notebook(sample_instances.values):\n",
    "        instance_no += 1    \n",
    "        print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "        #if cls_method == \"xgboost\":\n",
    "        instance = instance.reshape(1, -1)\n",
    "        pred = cls.predict(instance)\n",
    "\n",
    "        #Get Tree SHAP explanations for instance\n",
    "        exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "        feat_pres = []\n",
    "        feat_weights = []\n",
    "\n",
    "        for iteration in rel_exp:\n",
    "            #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "            presence_list = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "\n",
    "                for explanation in iteration:\n",
    "                    if each in explanation[0]:\n",
    "                        presence_list[list_idx] = 1\n",
    "\n",
    "            feat_pres.append(presence_list)\n",
    "\n",
    "        for iteration in exp:\n",
    "            #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "            weights = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "\n",
    "                for explanation in iteration:\n",
    "                    if each in explanation[0]:\n",
    "\n",
    "                        weights[list_idx] = explanation[1]\n",
    "            feat_weights.append(weights)\n",
    "\n",
    "        stability = st.getStability(feat_pres)\n",
    "        print (\"Stability:\", round(stability,2))\n",
    "        subset_stability.append(stability)\n",
    "\n",
    "        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "        avg_dispersal = 1-np.mean(rel_var)\n",
    "        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "        weight_stability.append(avg_dispersal)\n",
    "        adj_dispersal = 1-np.mean(second_var)\n",
    "        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "        adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    results[\"SHAP Subset Stability\"] = subset_stability\n",
    "    results[\"SHAP Weight Stability\"] = weight_stability\n",
    "    results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction Probability</th>\n",
       "      <th>SHAP Subset Stability</th>\n",
       "      <th>SHAP Weight Stability</th>\n",
       "      <th>SHAP Adjusted Weight Stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Prediction  Prediction Probability  SHAP Subset Stability  \\\n",
       "0        0           0                0.997319                    1.0   \n",
       "1        0           0                0.997458                    1.0   \n",
       "2        0           0                0.997196                    1.0   \n",
       "3        0           0                0.997522                    1.0   \n",
       "4        0           0                0.997529                    1.0   \n",
       "..     ...         ...                     ...                    ...   \n",
       "95       1           1                0.997351                    1.0   \n",
       "96       1           1                0.997485                    1.0   \n",
       "97       1           1                0.997405                    1.0   \n",
       "98       1           1                0.997315                    1.0   \n",
       "99       1           1                0.997434                    1.0   \n",
       "\n",
       "    SHAP Weight Stability  SHAP Adjusted Weight Stability  \n",
       "0                     1.0                             1.0  \n",
       "1                     1.0                             1.0  \n",
       "2                     1.0                             1.0  \n",
       "3                     1.0                             1.0  \n",
       "4                     1.0                             1.0  \n",
       "..                    ...                             ...  \n",
       "95                    1.0                             1.0  \n",
       "96                    1.0                             1.0  \n",
       "97                    1.0                             1.0  \n",
       "98                    1.0                             1.0  \n",
       "99                    1.0                             1.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(dataset_path, cls_method, \"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LIME\":\n",
    "    \n",
    "    feat_list = trainingdata.columns.tolist()\n",
    "    class_names = [\"Negative\", \"Positive\"]\n",
    "\n",
    "    cats = col_dict[\"discrete\"]\n",
    "\n",
    "    subset_stability = []\n",
    "    weight_stability = []\n",
    "    adjusted_weight_stability = []\n",
    "\n",
    "    #create explainer now that can be passed later\n",
    "    lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata.values,\n",
    "                          feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "\n",
    "    instance_no = 0\n",
    "    print(len(sample_instances))\n",
    "    #explain the chosen instances and find the stability score\n",
    "    for instance in tqdm_notebook(sample_instances.values):\n",
    "        instance_no += 1\n",
    "\n",
    "        print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "        #Get lime explanations for instance\n",
    "        feat_pres = []\n",
    "        feat_weights = []\n",
    "\n",
    "        for iteration in list(range(exp_iter)):\n",
    "\n",
    "            lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                                  max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "            all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "            bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "\n",
    "            presence_list = [0]*len(feat_list)\n",
    "            weights = [0]*len(feat_list)\n",
    "\n",
    "            for each in feat_list:\n",
    "                list_idx = feat_list.index(each)\n",
    "                #print (\"Feature\", list_idx)\n",
    "                for explanation in lime_exp.as_list():\n",
    "                    if each in explanation[0]:\n",
    "                        if explanation[1] > q1_min:\n",
    "                            presence_list[list_idx] = 1\n",
    "                        weights[list_idx] = explanation[1]\n",
    "\n",
    "            feat_pres.append(presence_list)\n",
    "            feat_weights.append(weights)\n",
    "\n",
    "        stability = st.getStability(feat_pres)\n",
    "        print (\"Stability:\", round(stability,2))\n",
    "        subset_stability.append(stability)\n",
    "\n",
    "        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "        avg_dispersal = 1-np.mean(rel_var)\n",
    "        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "        weight_stability.append(avg_dispersal)\n",
    "        adj_dispersal = 1-np.mean(second_var)\n",
    "        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "        adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    results[\"LIME Subset Stability\"] = subset_stability\n",
    "    results[\"LIME Weight Stability\"] = weight_stability\n",
    "    results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction Probability</th>\n",
       "      <th>SHAP Subset Stability</th>\n",
       "      <th>SHAP Weight Stability</th>\n",
       "      <th>SHAP Adjusted Weight Stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Prediction  Prediction Probability  SHAP Subset Stability  \\\n",
       "0        0           0                0.997319                    1.0   \n",
       "1        0           0                0.997458                    1.0   \n",
       "2        0           0                0.997196                    1.0   \n",
       "3        0           0                0.997522                    1.0   \n",
       "4        0           0                0.997529                    1.0   \n",
       "..     ...         ...                     ...                    ...   \n",
       "95       1           1                0.997351                    1.0   \n",
       "96       1           1                0.997485                    1.0   \n",
       "97       1           1                0.997405                    1.0   \n",
       "98       1           1                0.997315                    1.0   \n",
       "99       1           1                0.997434                    1.0   \n",
       "\n",
       "    SHAP Weight Stability  SHAP Adjusted Weight Stability  \n",
       "0                     1.0                             1.0  \n",
       "1                     1.0                             1.0  \n",
       "2                     1.0                             1.0  \n",
       "3                     1.0                             1.0  \n",
       "4                     1.0                             1.0  \n",
       "..                    ...                             ...  \n",
       "95                    1.0                             1.0  \n",
       "96                    1.0                             1.0  \n",
       "97                    1.0                             1.0  \n",
       "98                    1.0                             1.0  \n",
       "99                    1.0                             1.0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(dataset_path, cls_method, \"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3de09b36e3cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestingdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainingdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_cls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_samples' is not defined"
     ]
    }
   ],
   "source": [
    "testingdata = all_samples[0]\n",
    "instance = all_samples[0][0]\n",
    "trainingdata = all_train[0]\n",
    "\n",
    "cls = all_cls[0]\n",
    "scaler = all_scalers[0]\n",
    "\n",
    "feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names)\n",
    "if scaler == None:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                                 cls.predict_proba, num_features=10, labels=[0,1], top_labels=1)\n",
    "else:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                             scale_predict_fn, num_features=max_feat, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(cls)\n",
    "explanation = shap_explainer(testingdata)\n",
    "explanation.feature_names = feat_list\n",
    "shap.plots.waterfall(explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(scaler.inverse_transform(trainingdata), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)\n",
    "dataset_manager.static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.dynamic_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.static_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols for feat in range(len(feat_list)) if col in feat_list[feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(feat_list)[cats]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
