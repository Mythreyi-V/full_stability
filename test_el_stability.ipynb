{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show acv-exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#from acv_explainers import ACXplainer\n",
    "from learning import *\n",
    "import pyAgrum\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))[0]\n",
    "            #print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[i]\n",
    "            abs_val = abs(shap_values[i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] >= q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter):\n",
    "    instance = instance.reshape(1, -1)\n",
    "    y = cls.predict(instance)\n",
    "    \n",
    "    t=np.var(y_train)\n",
    "\n",
    "    feats = []\n",
    "    feat_imp = []\n",
    "\n",
    "    for i in range(exp_iter):\n",
    "        sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train,\n",
    "                                                                                 t=t, pi_level=0.8)\n",
    "        clean_expl = sufficient_expl.copy()\n",
    "        clean_expl = clean_expl[0]\n",
    "        clean_expl = [sublist for sublist in clean_expl if sum(n<0 for n in sublist)==0 ]\n",
    "\n",
    "        clean_sdp = sdp_expl[0].copy()\n",
    "        clean_sdp = [sdp for sdp in clean_sdp if sdp > 0]\n",
    "        \n",
    "        lximp = explainer.compute_local_sdp(X_train.shape[1], clean_expl)\n",
    "        feat_imp.append(lximp)\n",
    "        \n",
    "        if len(clean_expl)==0 or len(clean_expl[0])==0:\n",
    "            print(\"No explamation meets pi level\")\n",
    "        else:\n",
    "            lens = [len(i) for i in clean_expl]\n",
    "            me_loc = [i for i in range(len(lens)) if lens[i]==min(lens)]\n",
    "            mse_loc = np.argmax(np.array(clean_sdp)[me_loc])\n",
    "            mse = np.array(clean_expl)[me_loc][mse_loc]\n",
    "            feats.extend(mse)\n",
    "\n",
    "    if len(feats)==0:\n",
    "        feat_pos = []\n",
    "    else:\n",
    "        feat_pos = set(feats)\n",
    "    \n",
    "      \n",
    "    feat_imp = np.mean(feat_imp, axis=0)\n",
    "    \n",
    "    return feat_imp, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list, percentile):\n",
    "    label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "    feat_pos = []\n",
    "    lkhoods = []\n",
    "    \n",
    "    save_to = os.path.join(PATH, dataset, cls_method, method_name)+\"/\"\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        [bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                           None, scaler, cls, save_to, dataset, show_in_notebook = False,\n",
    "                                                           samples=round(len(feat_list)*2))\n",
    "        \n",
    "        ie = pyAgrum.LazyPropagation(bn)\n",
    "        result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "        if len(result_posterior.shape)==1:\n",
    "            result_proba = result_posterior.values[0]\n",
    "        else:\n",
    "            result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]        \n",
    "        row = instance['original_vector']\n",
    "        #print(row)\n",
    "\n",
    "        likelihood = [0]*len(feat_list)\n",
    "\n",
    "        for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "\n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "\n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "            \n",
    "            result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "            if len(result_posterior.shape)==1:\n",
    "                new_proba = result_posterior.values[0]\n",
    "            else:\n",
    "                new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "            #print(result_proba, new_proba)\n",
    "            proba_change = result_proba-new_proba\n",
    "            likelihood[j] = abs(proba_change)\n",
    "\n",
    "        lkhoods.append(likelihood)\n",
    "        \n",
    "    min_coef = min( np.mean(lkhoods, axis=0))\n",
    "    max_coef = max( np.mean(lkhoods, axis=0))\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "\n",
    "    #If fixing all features produces the same result for the class,\n",
    "    #return all features\n",
    "    if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "        feat_pos.extend(range(len(feat_list)))\n",
    "    else:\n",
    "        feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "    feat_pos = set(feat_pos)\n",
    "        \n",
    "    return np.mean(lkhoods, axis=0), feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['production']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"production\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"prefix\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"logit\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "xai_method = \"LINDA\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\": [\"production\"] \n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in datasets:\n",
    "\n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = max_prefix\n",
    "\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "#     #data = dataset_manager.read_dataset()\n",
    "\n",
    "#     all_pipelines = []\n",
    "#     all_cls = []\n",
    "#     all_encoders = []\n",
    "#     all_scalers = []\n",
    "#     all_train = []\n",
    "#     all_samples = []\n",
    "#     all_results = []\n",
    "\n",
    "#     dt_val_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.concat([dt_train_prefixes, dt_val_prefixes])\n",
    "#     dt_train_prefixes = dataset_manager.generate_prefix_data(dt_train_prefixes, min_prefix_length, max_prefix_length)\n",
    "\n",
    "#     if bucket_method == \"state\":\n",
    "#         bucket_encoding = \"last\"\n",
    "#     else:\n",
    "#         bucket_encoding = \"agg\"\n",
    "    \n",
    "#     bucketer_args = {'encoding_method':bucket_encoding,\n",
    "#                      'case_id_col':dataset_manager.case_id_col, \n",
    "#                      'cat_cols':[dataset_manager.activity_col], \n",
    "#                      'num_cols':[], \n",
    "#                      'random_state':random_state}\n",
    "\n",
    "#     bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "#     bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "    \n",
    "#     for ii in range(n_iter):\n",
    "#         num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "#         for bucket in tqdm_notebook(range(num_buckets)):\n",
    "#             bucketID = bucket+1\n",
    "#             print ('Bucket', bucketID)\n",
    "\n",
    "#             #import everything needed to sort and predict\n",
    "#             pipeline_path = os.path.join(PATH, \"%s/%s/%s/pipelines/pipeline_bucket_%s.joblib\" % \n",
    "#                                          (dataset_ref, cls_method, method_name, bucketID))\n",
    "#             pipeline = joblib.load(pipeline_path)\n",
    "#             feature_combiner = pipeline['encoder']\n",
    "#             if 'scaler' in pipeline.named_steps:\n",
    "#                 scaler = pipeline['scaler']\n",
    "#             else:\n",
    "#                 scaler = None\n",
    "#             cls = pipeline['cls']\n",
    "            \n",
    "#             all_cls.append(cls)\n",
    "#             all_encoders.append(feature_combiner)\n",
    "#             all_scalers.append(scaler)\n",
    "#             all_pipelines.append(pipeline)\n",
    "\n",
    "#             #find relevant samples for bucket\n",
    "#             bucket_sample = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             results_template = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID)))\n",
    "    \n",
    "#             if scaler != None:\n",
    "#                 bucket_sample = scaler.transform(bucket_sample)\n",
    "#             bucket_results = results_template\n",
    "            \n",
    "#             feat_names = feature_combiner.get_feature_names()\n",
    "#             feat_list = [feat.replace(\" \", \"_\") for feat in feat_names]\n",
    "            \n",
    "#             all_samples.append(bucket_sample)\n",
    "#             all_results.append(bucket_results)\n",
    "            \n",
    "#             #import training data for bucket\n",
    "#             train_data = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "#                                                           (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             if scaler != None:\n",
    "#                 train_data = scaler.transform(train_data)\n",
    "            \n",
    "#             all_train.append(train_data)\n",
    "            \n",
    "#             #get target values for bucket\n",
    "#             train_prefixes = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "if xai_method==\"SHAP\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                        (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in tqdm_notebook(range(num_buckets)):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "                \n",
    "                #import everything needed to sort and predict\n",
    "                pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                             (bucketID))\n",
    "                pipeline = joblib.load(pipeline_path)\n",
    "                feature_combiner = pipeline['encoder']\n",
    "                if 'scaler' in pipeline.named_steps:\n",
    "                    scaler = pipeline['scaler']\n",
    "                else:\n",
    "                    scaler = None\n",
    "                cls = pipeline['cls']\n",
    "                \n",
    "                #import training data for bucket\n",
    "                trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                if scaler != None:\n",
    "                    trainingdata = scaler.transform(trainingdata)\n",
    "                    \n",
    "                #find relevant samples for bucket\n",
    "                sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID)))\n",
    "                \n",
    "                if scaler != None:\n",
    "                    sample_instances = scaler.transform(sample_instances)\n",
    "                \n",
    "                #create explanation mechanism\n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    shap_explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "                print(type(shap_explainer))\n",
    "                \n",
    "                #Identify feature names\n",
    "                feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "                \n",
    "                subset_stability = []\n",
    "                weight_stability = []\n",
    "                adjusted_weight_stability = []\n",
    "                    \n",
    "                #explain the chosen instances and find the stability score\n",
    "                instance_no = 0\n",
    "                for instance in tqdm_notebook(sample_instances[:1]):\n",
    "                    instance_no += 1    \n",
    "                    print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "                    \n",
    "                    #if cls_method == \"xgboost\":\n",
    "                    instance = instance.reshape(1, -1)\n",
    "                    pred = cls.predict(instance)\n",
    "\n",
    "                    #Get Tree SHAP explanations for instance\n",
    "                    exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "                    feat_pres = []\n",
    "                    feat_weights = []\n",
    "\n",
    "                    for iteration in rel_exp:\n",
    "                        #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "                        presence_list = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "                                    presence_list[list_idx] = 1\n",
    "\n",
    "                        feat_pres.append(presence_list)\n",
    "\n",
    "                    for iteration in exp:\n",
    "                        #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "                        weights = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "\n",
    "                                    weights[list_idx] = explanation[1]\n",
    "                        feat_weights.append(weights)\n",
    "\n",
    "                    stability = st.getStability(feat_pres)\n",
    "                    print (\"Stability:\", round(stability,2))\n",
    "                    subset_stability.append(stability)\n",
    "\n",
    "                    rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                    avg_dispersal = 1-np.mean(rel_var)\n",
    "                    print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                    weight_stability.append(avg_dispersal)\n",
    "                    adj_dispersal = 1-np.mean(second_var)\n",
    "                    print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                    adjusted_weight_stability.append(adj_dispersal)\n",
    "                    \n",
    "                results[\"SHAP Subset Stability\"] = subset_stability\n",
    "                results[\"SHAP Weight Stability\"] = weight_stability\n",
    "                results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "                results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "                all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "#                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LIME\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(9, num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "           #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(sample_instances):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "\n",
    "                    lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                                          max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "                    all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "                    bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = [0]*len(feat_list)\n",
    "                    weights = [0]*len(feat_list)\n",
    "\n",
    "                    for each in feat_list:\n",
    "                        list_idx = feat_list.index(each)\n",
    "                        #print (\"Feature\", list_idx)\n",
    "                        for explanation in lime_exp.as_list():\n",
    "                            if each in explanation[0]:\n",
    "                                if explanation[1] >= q1_min:\n",
    "                                    presence_list[list_idx] = 1\n",
    "                                weights[list_idx] = explanation[1]\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LIME Subset Stability\"] = subset_stability\n",
    "            results[\"LIME Weight Stability\"] = weight_stability\n",
    "            results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "            all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "#                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"ACV\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "            \n",
    "            acv_explainer = joblib.load(os.path.join(PATH,'%s/%s/%s/acv_surrogate/acv_explainer_bucket_%s.joblib'% \n",
    "                                                                    (dataset_ref, cls_method, method_name, bucketID)))\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(sample_instances[:1]):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_acv_features(acv_explainer, instance, cls, trainingdata, targets, 1)\n",
    "                    print(weights)\n",
    "                    print(feat_pos)\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "#             results[\"ACV Subset Stability\"] = subset_stability\n",
    "#             results[\"ACV Weight Stability\"] = weight_stability\n",
    "#             results[\"ACV Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "#             results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "#                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "#                 all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2a7a7bf39342e8b88a0aa36632d5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff3f53c148b42dcb77201f15accbe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 2\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a2e79bc3bb449a8af4c7ef02351c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 3\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeac37cd75f34bcfaf75ad2da530d91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 4\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63c90951447458ea4225f4303366708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 5\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe90fa01613f4b39b1771c9d95790a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 6\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1181c50c48a84ad4bc6d52563fa80bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "\n",
      "Bucket 7\n",
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e518fd9044f07b89aa702c0a3f15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "Stability: nan\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 8 .\n",
      "Selecting Greedy Hill Climbing Algorithm\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfBounds",
     "evalue": "[pyAgrum] Out of bound error: label '\"(1823.94, 2240.52]\"' is unknown in agg__activity_duration_std<\"(106.75, 179.62]\",\"(213.7, 247.78]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfBounds\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m\n\u001b[0;32m     67\u001b[0m feat_weights \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(exp_iter)):\n\u001b[1;32m---> 72\u001b[0m     weights, feat_pos \u001b[38;5;241m=\u001b[39m \u001b[43mget_linda_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m#print(weights)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m#print(feat_pos)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     feat_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(feat_pos)\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mget_linda_features\u001b[1;34m(instance, cls, scaler, dataset, exp_iter, feat_list, percentile)\u001b[0m\n\u001b[0;32m     41\u001b[0m         feat_bin \u001b[38;5;241m=\u001b[39m str_bins[k]\n\u001b[0;32m     43\u001b[0m ie \u001b[38;5;241m=\u001b[39m pyAgrum\u001b[38;5;241m.\u001b[39mLazyPropagation(bn)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetEvidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_bin\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m ie\u001b[38;5;241m.\u001b[39mmakeInference()\n\u001b[0;32m     47\u001b[0m result_posterior \u001b[38;5;241m=\u001b[39m ie\u001b[38;5;241m.\u001b[39mposterior(bn\u001b[38;5;241m.\u001b[39midFromName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtopandas()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyAgrum\\pyAgrum.py:13238\u001b[0m, in \u001b[0;36mLazyPropagation.setEvidence\u001b[1;34m(self, evidces)\u001b[0m\n\u001b[0;32m  13236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meraseAllEvidence()\n\u001b[0;32m  13237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m evidces\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m> 13238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddEvidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyAgrum\\pyAgrum.py:13516\u001b[0m, in \u001b[0;36mLazyPropagation.addEvidence\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m  13478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddEvidence\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  13479\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  13480\u001b[0m \u001b[38;5;124;03m    addEvidence(LazyPropagation self, gum::NodeId const id, gum::Idx const val)\u001b[39;00m\n\u001b[0;32m  13481\u001b[0m \u001b[38;5;124;03m    addEvidence(LazyPropagation self, std::string const & nodeName, gum::Idx const val)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  13514\u001b[0m \n\u001b[0;32m  13515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 13516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pyAgrum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLazyPropagation_addEvidence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfBounds\u001b[0m: [pyAgrum] Out of bound error: label '\"(1823.94, 2240.52]\"' is unknown in agg__activity_duration_std<\"(106.75, 179.62]\",\"(213.7, 247.78]\">"
     ]
    }
   ],
   "source": [
    "if xai_method==\"LINDA\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            test_dict = generate_local_predictions( sample_instances, results[\"Actual\"], cls, scaler, None )\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(test_dict):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)\n",
    "                    #print(weights)\n",
    "                    #print(feat_pos)\n",
    "                    \n",
    "                    feat_pos = list(feat_pos)\n",
    "                    \n",
    "                    #bins = pd.cut(weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    #q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "\n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "                \n",
    "                #print(feat_pres)\n",
    "                #print(feat_weights)\n",
    "                \n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LINDA Subset Stability\"] = subset_stability\n",
    "            results[\"LINDA Weight Stability\"] = weight_stability\n",
    "            results[\"LINDA Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "            all_results.append(results)\n",
    "            \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Greedy Hill Climbing Algorithm\n"
     ]
    }
   ],
   "source": [
    "label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "feat_pos = []\n",
    "lkhoods = []\n",
    "\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)+\"/\"\n",
    "\n",
    "[bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                       None, scaler, cls, save_to, dataset_ref, show_in_notebook = False,\n",
    "                                                       samples=round(len(feat_list)*1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = pyAgrum.LazyPropagation(bn)\n",
    "result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "\n",
    "if len(result_posterior.shape)==1:\n",
    "    result_proba = result_posterior.values[0]\n",
    "else:\n",
    "    result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "\n",
    "row = instance['original_vector']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['static__Work_Order_Qty',\n",
       " 'static__Part_Desc__Adjusting_Nut',\n",
       " 'static__Part_Desc__Ballnut',\n",
       " 'static__Part_Desc__Barrel',\n",
       " 'static__Part_Desc__Bearing',\n",
       " 'static__Part_Desc__Cable_Head',\n",
       " 'static__Part_Desc__Cast',\n",
       " 'static__Part_Desc__Clamp',\n",
       " 'static__Part_Desc__Compound_Dies',\n",
       " 'static__Part_Desc__Cutting_Wheel',\n",
       " 'static__Part_Desc__Drill',\n",
       " 'static__Part_Desc__Drum',\n",
       " 'static__Part_Desc__Electrical_Contact',\n",
       " 'static__Part_Desc__Fastener',\n",
       " 'static__Part_Desc__Flange',\n",
       " 'static__Part_Desc__Flange_Bolt',\n",
       " 'static__Part_Desc__Gauge',\n",
       " 'static__Part_Desc__Hinge',\n",
       " 'static__Part_Desc__Housing',\n",
       " 'static__Part_Desc__Locker',\n",
       " 'static__Part_Desc__O-Ring',\n",
       " 'static__Part_Desc__Piston',\n",
       " 'static__Part_Desc__Plug',\n",
       " 'static__Part_Desc__Punch_Holder',\n",
       " 'static__Part_Desc__Punch_Plate',\n",
       " 'static__Part_Desc__Slip_Ring_Adapter',\n",
       " 'static__Part_Desc__Socket',\n",
       " 'static__Part_Desc__Spinner',\n",
       " 'static__Part_Desc__Spur_Gear',\n",
       " 'static__Part_Desc__Tube',\n",
       " 'static__Part_Desc__Wheel_Shaft',\n",
       " 'static__Part_Desc__other',\n",
       " 'static__Rework_missing',\n",
       " 'agg__Activity_Final_Inspection_Q.C.',\n",
       " 'agg__Activity_Flat_Grinding_-_Machine_11',\n",
       " 'agg__Activity_Grinding_Rework_-_Machine_27',\n",
       " 'agg__Activity_Lapping_-_Machine_1',\n",
       " 'agg__Activity_Laser_Marking_-_Machine_7',\n",
       " 'agg__Activity_Milling_-_Machine_16',\n",
       " 'agg__Activity_Packing',\n",
       " 'agg__Activity_Round_Grinding_-_Machine_12',\n",
       " 'agg__Activity_Round_Grinding_-_Machine_2',\n",
       " 'agg__Activity_Round_Grinding_-_Machine_3',\n",
       " 'agg__Activity_Round_Grinding_-_Manual',\n",
       " 'agg__Activity_Round_Grinding_-_Q.C.',\n",
       " 'agg__Activity_Turn_&_Mill._&_Screw_Assem_-_Machine_10',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_10',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_4',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_5',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_6',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_8',\n",
       " 'agg__Activity_Turning_&_Milling_-_Machine_9',\n",
       " 'agg__Activity_Turning_&_Milling_Q.C.',\n",
       " 'agg__Activity_Turning_-_Machine_8',\n",
       " 'agg__Activity_Turning_-_Machine_9',\n",
       " 'agg__Activity_Turning_Q.C.',\n",
       " 'agg__Activity_Wire_Cut_-_Machine_13',\n",
       " 'agg__Activity_other',\n",
       " 'agg__Resource_ID0997',\n",
       " 'agg__Resource_ID0998',\n",
       " 'agg__Resource_ID3767',\n",
       " 'agg__Resource_ID3846',\n",
       " 'agg__Resource_ID4132',\n",
       " 'agg__Resource_ID4163',\n",
       " 'agg__Resource_ID4167',\n",
       " 'agg__Resource_ID4219',\n",
       " 'agg__Resource_ID4287',\n",
       " 'agg__Resource_ID4326',\n",
       " 'agg__Resource_ID4355',\n",
       " 'agg__Resource_ID4385',\n",
       " 'agg__Resource_ID4429',\n",
       " 'agg__Resource_ID4445',\n",
       " 'agg__Resource_ID4493',\n",
       " 'agg__Resource_ID4528',\n",
       " 'agg__Resource_ID4529',\n",
       " 'agg__Resource_ID4618',\n",
       " 'agg__Resource_ID4641',\n",
       " 'agg__Resource_ID4718',\n",
       " 'agg__Resource_ID4794',\n",
       " 'agg__Resource_ID4820',\n",
       " 'agg__Resource_ID4872',\n",
       " 'agg__Resource_ID4882',\n",
       " 'agg__Resource_ID4932',\n",
       " 'agg__Resource_other',\n",
       " 'agg__Report_Type_B',\n",
       " 'agg__Report_Type_D',\n",
       " 'agg__Report_Type_S',\n",
       " 'agg__Resource.1_Machine_1_-_Lapping',\n",
       " 'agg__Resource.1_Machine_10_-_Grinding',\n",
       " 'agg__Resource.1_Machine_11_-_Grinding',\n",
       " 'agg__Resource.1_Machine_12_-_Grinding',\n",
       " 'agg__Resource.1_Machine_15_-_Turning',\n",
       " 'agg__Resource.1_Machine_16_-_Milling',\n",
       " 'agg__Resource.1_Machine_2_-_Round_Grinding',\n",
       " 'agg__Resource.1_Machine_27_-_Grinding',\n",
       " 'agg__Resource.1_Machine_3_-_Round_Grinding',\n",
       " 'agg__Resource.1_Machine_4_-_Turning_&_Milling',\n",
       " 'agg__Resource.1_Machine_5_-_Turning_&_Milling',\n",
       " 'agg__Resource.1_Machine_6_-_Turning_&_Milling',\n",
       " 'agg__Resource.1_Machine_7-_Laser_Marking',\n",
       " 'agg__Resource.1_Machine_8_-_Turning_&_Milling',\n",
       " 'agg__Resource.1_Machine_9_-_Turning_&_Milling',\n",
       " 'agg__Resource.1_Packing',\n",
       " 'agg__Resource.1_Quality_Check_1',\n",
       " 'agg__Resource.1_Wire_Cut_-_Machine_13',\n",
       " 'agg__Resource.1_other',\n",
       " 'agg__Qty_Completed_mean',\n",
       " 'agg__Qty_Completed_max',\n",
       " 'agg__Qty_Completed_min',\n",
       " 'agg__Qty_Completed_sum',\n",
       " 'agg__Qty_Completed_std',\n",
       " 'agg__Qty_for_MRB_mean',\n",
       " 'agg__Qty_for_MRB_max',\n",
       " 'agg__Qty_for_MRB_min',\n",
       " 'agg__Qty_for_MRB_sum',\n",
       " 'agg__Qty_for_MRB_std',\n",
       " 'agg__activity_duration_mean',\n",
       " 'agg__activity_duration_max',\n",
       " 'agg__activity_duration_min',\n",
       " 'agg__activity_duration_sum',\n",
       " 'agg__activity_duration_std',\n",
       " 'agg__hour_mean',\n",
       " 'agg__hour_max',\n",
       " 'agg__hour_min',\n",
       " 'agg__hour_sum',\n",
       " 'agg__hour_std',\n",
       " 'agg__weekday_mean',\n",
       " 'agg__weekday_max',\n",
       " 'agg__weekday_min',\n",
       " 'agg__weekday_sum',\n",
       " 'agg__weekday_std',\n",
       " 'agg__month_mean',\n",
       " 'agg__month_max',\n",
       " 'agg__month_min',\n",
       " 'agg__month_sum',\n",
       " 'agg__month_std',\n",
       " 'agg__timesincemidnight_mean',\n",
       " 'agg__timesincemidnight_max',\n",
       " 'agg__timesincemidnight_min',\n",
       " 'agg__timesincemidnight_sum',\n",
       " 'agg__timesincemidnight_std',\n",
       " 'agg__timesincelastevent_mean',\n",
       " 'agg__timesincelastevent_max',\n",
       " 'agg__timesincelastevent_min',\n",
       " 'agg__timesincelastevent_sum',\n",
       " 'agg__timesincelastevent_std',\n",
       " 'agg__timesincecasestart_mean',\n",
       " 'agg__timesincecasestart_max',\n",
       " 'agg__timesincecasestart_min',\n",
       " 'agg__timesincecasestart_sum',\n",
       " 'agg__timesincecasestart_std',\n",
       " 'agg__event_nr_mean',\n",
       " 'agg__event_nr_max',\n",
       " 'agg__event_nr_min',\n",
       " 'agg__event_nr_sum',\n",
       " 'agg__event_nr_std',\n",
       " 'agg__open_cases_mean',\n",
       " 'agg__open_cases_max',\n",
       " 'agg__open_cases_min',\n",
       " 'agg__open_cases_sum',\n",
       " 'agg__open_cases_std']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.0\n",
      "\"(57.06, 276.51]\"\n",
      "0.0\n",
      "\"(0.024, 0.23]\"\n",
      "0.0\n",
      "\"(0.12, 0.23]\"\n",
      "0.0\n",
      "\"(-0.01, 0.2]\"\n",
      "0.0\n",
      "\"(0.011, 0.21]\"\n",
      "1.0\n",
      "\"(0.81, 1.0]\"\n",
      "0.0\n",
      "\"(-0.01, 0.12]\"\n",
      "0.0\n",
      "\"(0.16, 0.17]\"\n",
      "0.0\n",
      "\"(0.1, 0.25]\"\n",
      "0.0\n",
      "\"(-0.01, 0.21]\"\n",
      "0.0\n",
      "\"(0.12, 0.25]\"\n",
      "0.0\n",
      "\"(-0.01, 0.25]\"\n",
      "0.0\n",
      "\"(-0.01, 0.2]\"\n",
      "0.0\n",
      "\"(0.026, 0.16]\"\n",
      "0.0\n",
      "\"(-0.01, 0.21]\"\n",
      "0.0\n",
      "\"(-0.01, 0.003]\"\n",
      "0.0\n",
      "\"(0.045, 0.18]\"\n",
      "0.0\n",
      "\"(0.0074, 0.029]\"\n",
      "0.0\n",
      "\"(-0.01, 0.22]\"\n",
      "0.0\n",
      "\"(0.18, 0.2]\"\n",
      "0.0\n",
      "\"(0.14, 0.19]\"\n",
      "0.0\n",
      "\"(-0.01, 0.24]\"\n",
      "0.0\n",
      "\"(0.092, 0.18]\"\n",
      "0.0\n",
      "\"(0.11, 0.14]\"\n",
      "0.0\n",
      "\"(0.15, 0.18]\"\n",
      "0.0\n",
      "\"(-0.01, 0.19]\"\n",
      "0.0\n",
      "\"(0.056, 0.24]\"\n",
      "0.0\n",
      "\"(-0.01, 0.048]\"\n",
      "0.0\n",
      "\"(0.11, 0.18]\"\n",
      "0.0\n",
      "\"(0.17, 0.25]\"\n",
      "0.0\n",
      "\"(0.058, 0.23]\"\n",
      "0.0\n",
      "\"(0.18, 0.21]\"\n",
      "1.0\n",
      "\"(1.06, 1.15]\"\n",
      "0.0\n",
      "\"(-0.01, 0.34]\"\n",
      "0.0\n",
      "\"(-0.01, 0.72]\"\n",
      "0.0\n",
      "\"(-0.01, 0.26]\"\n",
      "0.0\n",
      "\"(-0.01, 1.01]\"\n",
      "0.0\n",
      "\"(0.18, 0.3]\"\n",
      "0.0\n",
      "\"(-0.01, 0.063]\"\n",
      "0.0\n",
      "\"(0.25, 0.41]\"\n",
      "0.0\n",
      "\"(0.4, 0.57]\"\n",
      "0.0\n",
      "\"(0.16, 0.48]\"\n",
      "0.0\n",
      "\"(0.16, 0.54]\"\n",
      "0.0\n",
      "\"(-0.01, 0.5]\"\n",
      "0.0\n",
      "\"(0.005, 0.19]\"\n",
      "0.0\n",
      "\"(-0.01, 1.7]\"\n",
      "0.0\n",
      "\"(-0.01, 1.56]\"\n",
      "0.0\n",
      "\"(0.99, 1.19]\"\n",
      "7.0\n",
      "\"(6.51, 7.0]\"\n",
      "0.0\n",
      "\"(0.34, 1.5]\"\n",
      "0.0\n",
      "\"(0.035, 1.5]\"\n",
      "0.0\n",
      "\"(0.86, 1.3]\"\n",
      "0.0\n",
      "\"(0.51, 0.58]\"\n",
      "0.0\n",
      "\"(-0.01, 1.54]\"\n",
      "0.0\n",
      "\"(-0.01, 1.49]\"\n",
      "0.0\n",
      "\"(0.04, 0.24]\"\n",
      "0.0\n",
      "\"(0.15, 0.24]\"\n",
      "0.0\n",
      "\"(0.0012, 0.15]\"\n",
      "0.0\n",
      "\"(0.65, 1.48]\"\n",
      "0.0\n",
      "\"(0.41, 0.68]\"\n",
      "0.0\n",
      "\"(-0.01, 0.23]\"\n",
      "0.0\n",
      "\"(0.43, 0.99]\"\n",
      "0.0\n",
      "\"(0.51, 0.63]\"\n",
      "0.0\n",
      "\"(-0.01, 0.47]\"\n",
      "0.0\n",
      "\"(0.18, 0.34]\"\n",
      "0.0\n",
      "\"(-0.01, 0.93]\"\n",
      "0.0\n",
      "\"(-0.01, 0.54]\"\n",
      "0.0\n",
      "\"(-0.01, 1.1]\"\n",
      "0.0\n",
      "\"(0.0078, 0.59]\"\n",
      "0.0\n",
      "\"(0.44, 0.49]\"\n",
      "2.0\n",
      "\"(1.07, 2.0]\"\n",
      "0.0\n",
      "\"(0.17, 0.33]\"\n",
      "0.0\n",
      "\"(-0.01, 0.32]\"\n",
      "0.0\n",
      "\"(1.14, 1.43]\"\n",
      "2.0\n",
      "\"(2.0, 2.67]\"\n",
      "0.0\n",
      "\"(0.21, 0.65]\"\n",
      "0.0\n",
      "\"(0.14, 0.73]\"\n",
      "1.0\n",
      "\"(0.83, 1.0]\"\n",
      "1.0\n",
      "\"(1.0, 1.31]\"\n",
      "0.0\n",
      "\"(0.23, 0.33]\"\n",
      "0.0\n",
      "\"(0.17, 0.32]\"\n",
      "0.0\n",
      "\"(0.13, 0.59]\"\n",
      "1.0\n",
      "\"(1.0, 1.26]\"\n",
      "0.0\n",
      "\"(-0.01, 0.51]\"\n",
      "0.0\n",
      "\"(-0.01, 0.46]\"\n",
      "5.0\n",
      "\"(4.26, 5.0]\"\n",
      "2.0\n",
      "\"(2.0, 2.6]\"\n",
      "0.0\n",
      "\"(0.67, 0.7]\"\n",
      "0.0\n",
      "\"(0.58, 1.11]\"\n",
      "0.0\n",
      "\"(-0.01, 0.67]\"\n",
      "0.0\n",
      "\"(0.11, 0.73]\"\n",
      "0.0\n",
      "\"(-0.01, 0.91]\"\n",
      "0.0\n",
      "\"(-0.01, 0.78]\"\n",
      "0.0\n",
      "\"(0.47, 0.55]\"\n",
      "0.0\n",
      "\"(0.32, 0.48]\"\n",
      "0.0\n",
      "\"(0.043, 0.47]\"\n",
      "0.0\n",
      "\"(-0.01, 0.77]\"\n",
      "7.0\n",
      "\"(5.78, 7.0]\"\n",
      "0.0\n",
      "\"(0.31, 0.91]\"\n",
      "0.0\n",
      "\"(0.23, 0.34]\"\n",
      "0.0\n",
      "\"(-0.01, 1.48]\"\n",
      "0.0\n",
      "\"(0.68, 1.32]\"\n",
      "0.0\n",
      "\"(-0.01, 0.24]\"\n",
      "0.0\n",
      "\"(-0.01, 0.73]\"\n",
      "0.0\n",
      "\"(-0.01, 0.23]\"\n",
      "0.0\n",
      "\"(-0.01, 0.7]\"\n",
      "21.428571\n",
      "\"(27.42, 37.99]\"\n",
      "75.0\n",
      "\"(75.01, 104.78]\"\n",
      "0.0\n",
      "\"(0.069, 0.12]\"\n",
      "150.0\n",
      "\"(149.96, 197.97]\"\n",
      "25.533358\n",
      "\"(27.58, 33.14]\"\n",
      "0.0\n",
      "\"(-0.01, 0.16]\"\n",
      "0.0\n",
      "\"(0.085, 0.23]\"\n",
      "0.0\n",
      "\"(-0.01, 0.18]\"\n",
      "0.0\n",
      "\"(0.11, 0.14]\"\n",
      "0.0\n",
      "\"(0.017, 0.13]\"\n",
      "260.571429\n",
      "\"(260.56, 296.73]\"\n",
      "596.0\n",
      "\"(355.84000000000003, 595.99]\"\n",
      "95.0\n",
      "\"(95.0, 131.73]\"\n",
      "1824.0\n",
      "\"(1823.94, 2240.52]\"\n",
      "179.630971\n",
      "\"(106.75, 179.62]\"\n",
      "12.285714\n",
      "\"(9.15, 12.29]\"\n",
      "21.0\n",
      "\"(20.99, 21.96]\"\n",
      "6.0\n",
      "\"(6.0, 7.37]\"\n",
      "86.0\n",
      "\"(85.99, 96.51]\"\n",
      "5.559205\n",
      "\"(5.56, 7.13]\"\n",
      "2.571429\n",
      "\"(2.56, 3.14]\"\n",
      "4.0\n",
      "\"(3.6100000000000003, 4.0]\"\n",
      "2.0\n",
      "\"(2.0, 2.6]\"\n",
      "18.0\n",
      "\"(17.99, 20.98]\"\n",
      "0.786796\n",
      "\"(0.007000000000000001, 0.79]\"\n",
      "1.0\n",
      "\"(0.99, 1.14]\"\n",
      "1.0\n",
      "\"(0.99, 1.25]\"\n",
      "1.0\n",
      "\"(0.99, 1.17]\"\n",
      "7.0\n",
      "\"(6.99, 10.19]\"\n",
      "0.0\n",
      "\"(-0.01, 0.092]\"\n",
      "758.857143\n",
      "\"(628.65, 758.86]\"\n",
      "1264.0\n",
      "\"(1173.73, 1264.0]\"\n",
      "387.0\n",
      "\"(218.27, 386.98]\"\n",
      "5312.0\n",
      "\"(4077.24, 5312.05]\"\n",
      "332.933641\n",
      "\"(332.93, 454.5]\"\n",
      "383.571429\n",
      "\"(383.78000000000003, 1721.72]\"\n",
      "1160.0\n",
      "\"(1162.07, 10863.05]\"\n",
      "0.0\n",
      "\"(-0.01, 0.06]\"\n",
      "2685.0\n",
      "\"(2686.54, 10075.41]\"\n",
      "392.323785\n",
      "\"(392.46000000000004, 3409.03]\"\n",
      "999.714286\n",
      "\"(999.29, 6982.04]\"\n",
      "2685.0\n",
      "\"(2686.54, 8930.22]\"\n",
      "0.0\n",
      "\"(-0.01, 0.12]\"\n",
      "6998.0\n",
      "\"(6995.07, 41254.44]\"\n",
      "911.469274\n",
      "\"(911.51, 4691.76]\"\n",
      "4.0\n",
      "\"(3.99, 4.1]\"\n",
      "7.0\n",
      "\"(6.99, 7.18]\"\n",
      "1.0\n",
      "\"(0.99, 1.2]\"\n",
      "28.0\n",
      "\"(27.99, 28.13]\"\n",
      "2.160247\n",
      "\"(2.1500000000000004, 2.3]\"\n",
      "29.714286\n",
      "\"(26.189999999999998, 29.71]\"\n",
      "30.0\n",
      "\"(29.99, 34.36]\"\n",
      "28.0\n",
      "\"(28.0, 32.26]\"\n",
      "208.0\n",
      "\"(196.02, 208.0]\"\n",
      "0.755929\n",
      "\"(0.75, 2.03]\"\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "            \n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "            \n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "                \n",
    "#                print(bins_diff)\n",
    "#                 for k in range(len(bins)):\n",
    "#                     if k!=len(bins)-1 and bins[k+1][0]-bins[k][1]!=0:\n",
    "#                         print(\"Gap between bins\")\n",
    "#                         lower = val - bins[k][1]\n",
    "#                         higher = bins[k+1][0] - val\n",
    "#                         if lower > higher:\n",
    "#                             feat_bin = str_bins[k+1]\n",
    "#                         else:\n",
    "#                             feat_bin = str_bins[k]\n",
    "#                 if feat_bin==None:\n",
    "#                     print(\"Some other issue with bins\")\n",
    "                    \n",
    "            print(val)\n",
    "            #print(row[j])\n",
    "            print(str_bins[k])\n",
    "\n",
    "            ie = pyAgrum.LazyPropagation(bn)\n",
    "            ie.setEvidence({feat_list[j]: feat_bin})\n",
    "            ie.makeInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins[k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.2880971e+01 -1.0971000e-02]\n",
      " [ 3.4069029e+01  6.8149029e+01]]\n"
     ]
    }
   ],
   "source": [
    "bins_diff = np.array(bins) - val\n",
    "print(bins_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_diff = np.array([[6, 3, 1], [2, 5, 6], [2, 0, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.630971"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance[\"original_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25360039509402693"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[0][\"scaled_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.630971"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.variable(feat_list[j-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showInference(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(trainingdata, columns=feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAgrum.lib.bn2graph import BN2dot, BNinference2dot\n",
    "\n",
    "g = BNinference2dot(bn, size='\"100,10!\"', engine=ie)\n",
    "\n",
    "g.write(\"bn2graph_test.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_posterior.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = pyAgrum.BNLearner(save_to+\"feature_permutations/bpic2012/false_negatives/0_permutations.csv\")\n",
    "bn = learner.learnBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = all_samples[0]\n",
    "instance = all_samples[0][0]\n",
    "trainingdata = all_train[0]\n",
    "\n",
    "cls = all_cls[0]\n",
    "scaler = all_scalers[0]\n",
    "\n",
    "feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names)\n",
    "if scaler == None:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                                 cls.predict_proba, num_features=10, labels=[0,1], top_labels=1)\n",
    "else:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                             scale_predict_fn, num_features=max_feat, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(cls)\n",
    "explanation = shap_explainer(testingdata)\n",
    "explanation.feature_names = feat_list\n",
    "shap.plots.waterfall(explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(scaler.inverse_transform(trainingdata), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)\n",
    "dataset_manager.static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.dynamic_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.static_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols for feat in range(len(feat_list)) if col in feat_list[feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(feat_list)[cats]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
