{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "from anchor import anchor_tabular\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))\n",
    "            print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[0][i]\n",
    "            abs_val = abs(shap_values[0][i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] > q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepsis_cases_1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"sepsis_cases\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_lime = True\n",
    "generate_shap = True\n",
    "generate_anchor = False\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\": [\"production\"] \n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "\n",
    "    min_prefix_length = 1\n",
    "\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "\n",
    "    all_pipelines = []\n",
    "    all_cls = []\n",
    "    all_encoders = []\n",
    "    all_scalers = []\n",
    "    all_train = []\n",
    "    all_samples = []\n",
    "    all_results = []\n",
    "    \n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "        for bucket in range(num_buckets):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, \"%s/%s/%s/pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (dataset_ref, cls_method, method_name, bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "            \n",
    "            all_cls.append(cls)\n",
    "            all_encoders.append(feature_combiner)\n",
    "            all_scalers.append(scaler)\n",
    "            all_pipelines.append(pipeline)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            bucket_sample = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results_template = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "    \n",
    "            if scaler != None:\n",
    "                bucket_sample = scaler.transform(bucket_sample)\n",
    "            bucket_results = results_template\n",
    "            \n",
    "            feat_names = feature_combiner.get_feature_names()\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feat_names]\n",
    "            \n",
    "            all_samples.append(bucket_sample)\n",
    "            all_results.append(bucket_results)\n",
    "            \n",
    "            #import training data for bucket\n",
    "            train_data = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                train_data = scaler.transform(train_data)\n",
    "            \n",
    "            all_train.append(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "<class 'shap.explainers._permutation.Permutation'>\n",
      "Testing 1 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 2 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 3 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 4 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 5 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 6 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 7 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 8 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 9 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n",
      "Testing 10 of 750 .\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "(1, 272)\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 1.0\n",
      "Dispersal with no outliers: 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (750)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fd0b9f13b7c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[0madjusted_weight_stability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_dispersal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SHAP Subset Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SHAP Weight Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SHAP Adjusted Weight Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjusted_weight_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (10) does not match length of index (750)"
     ]
    }
   ],
   "source": [
    "if generate_shap:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                        (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in range(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                cls = all_cls[bucket]\n",
    "                feature_combiner = all_encoders[bucket]\n",
    "                scaler = all_scalers[bucket]\n",
    "                trainingdata = all_train[bucket]\n",
    "                sample_instances = all_samples[bucket]\n",
    "                results = all_results[bucket]\n",
    "                \n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    shap_explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "                print(type(shap_explainer))\n",
    "                \n",
    "                feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "                \n",
    "                subset_stability = []\n",
    "                weight_stability = []\n",
    "                adjusted_weight_stability = []\n",
    "                    \n",
    "                #explain the chosen instances and find the stability score\n",
    "                instance_no = 0\n",
    "                for instance in sample_instances[:10]:\n",
    "                    instance_no += 1    \n",
    "                    print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "                    \n",
    "                    #if cls_method == \"xgboost\":\n",
    "                    instance = instance.reshape(1, -1)\n",
    "                    pred = cls.predict(instance)\n",
    "\n",
    "                    #Get Tree SHAP explanations for instance\n",
    "                    exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "                    feat_pres = []\n",
    "                    feat_weights = []\n",
    "\n",
    "                    for iteration in rel_exp:\n",
    "                        #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "                        presence_list = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "                                    presence_list[list_idx] = 1\n",
    "\n",
    "                        feat_pres.append(presence_list)\n",
    "\n",
    "                    for iteration in exp:\n",
    "                        #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "                        weights = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "\n",
    "                                    weights[list_idx] = explanation[1]\n",
    "                        feat_weights.append(weights)\n",
    "\n",
    "                    stability = st.getStability(feat_pres)\n",
    "                    print (\"Stability:\", round(stability,2))\n",
    "                    subset_stability.append(stability)\n",
    "\n",
    "                    rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                    avg_dispersal = 1-np.mean(rel_var)\n",
    "                    print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                    weight_stability.append(avg_dispersal)\n",
    "                    adj_dispersal = 1-np.mean(second_var)\n",
    "                    print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                    adjusted_weight_stability.append(adj_dispersal)\n",
    "                    \n",
    "                results[\"SHAP Subset Stability\"] = subset_stability\n",
    "                results[\"SHAP Weight Stability\"] = weight_stability\n",
    "                results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "                all_results[bucket] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "750\n",
      "Testing 1 of 750 .\n",
      "Stability: 0.21\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n",
      "Testing 2 of 750 .\n",
      "Stability: 0.2\n",
      "Dispersal of feature importance: 0.77\n",
      "Dispersal with no outliers: 0.77\n",
      "Testing 3 of 750 .\n",
      "Stability: 0.38\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n",
      "Testing 4 of 750 .\n",
      "Stability: 0.27\n",
      "Dispersal of feature importance: 0.83\n",
      "Dispersal with no outliers: 0.83\n",
      "Testing 5 of 750 .\n",
      "Stability: 0.23\n",
      "Dispersal of feature importance: 0.94\n",
      "Dispersal with no outliers: 0.94\n",
      "Testing 6 of 750 .\n",
      "Stability: 0.17\n",
      "Dispersal of feature importance: 0.92\n",
      "Dispersal with no outliers: 0.92\n",
      "Testing 7 of 750 .\n",
      "Stability: 0.67\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n",
      "Testing 8 of 750 .\n",
      "Stability: 0.24\n",
      "Dispersal of feature importance: 0.91\n",
      "Dispersal with no outliers: 0.91\n",
      "Testing 9 of 750 .\n",
      "Stability: 0.5\n",
      "Dispersal of feature importance: 0.94\n",
      "Dispersal with no outliers: 0.94\n",
      "Testing 10 of 750 .\n",
      "Stability: 0.77\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (750)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-91a8d0f09702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0madjusted_weight_stability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_dispersal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LIME Subset Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LIME Weight Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LIME Adjusted Weight Stability\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjusted_weight_stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (10) does not match length of index (750)"
     ]
    }
   ],
   "source": [
    "if generate_lime:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in range(num_buckets):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            cls = all_cls[bucket]\n",
    "            feature_combiner = all_encoders[bucket]\n",
    "            scaler = all_scalers[bucket]\n",
    "            trainingdata = all_train[bucket]\n",
    "            sample_instances = all_samples[bucket]\n",
    "            results = all_results[bucket]\n",
    "            pipeline = all_pipelines[bucket]            \n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in sample_instances[:10]:\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "\n",
    "                    lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                                          max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "                    all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "                    bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = [0]*len(feat_list)\n",
    "                    weights = [0]*len(feat_list)\n",
    "\n",
    "                    for each in feat_list:\n",
    "                        list_idx = feat_list.index(each)\n",
    "                        #print (\"Feature\", list_idx)\n",
    "                        for explanation in lime_exp.as_list():\n",
    "                            if each in explanation[0]:\n",
    "                                if explanation[1] > q1_min:\n",
    "                                    presence_list[list_idx] = 1\n",
    "                                weights[list_idx] = explanation[1]\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LIME Subset Stability\"] = subset_stability\n",
    "            results[\"LIME Weight Stability\"] = weight_stability\n",
    "            results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            all_results[bucket] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = all_samples[0]\n",
    "instance = all_samples[0][0]\n",
    "trainingdata = all_train[0]\n",
    "\n",
    "cls = all_cls[0]\n",
    "scaler = all_scalers[0]\n",
    "\n",
    "feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names)\n",
    "if scaler == None:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                                 cls.predict_proba, num_features=10, labels=[0,1], top_labels=1)\n",
    "else:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                             scale_predict_fn, num_features=max_feat, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(cls)\n",
    "explanation = shap_explainer(testingdata)\n",
    "explanation.feature_names = feat_list\n",
    "shap.plots.waterfall(explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8204, 272)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  20.        ,    0.        ,    0.        , ...,   57.        ,\n",
       "          57.        ,    0.        ],\n",
       "       [  20.        ,    0.        ,    0.        , ...,   57.        ,\n",
       "         114.        ,    0.        ],\n",
       "       [  20.        ,    0.        ,    0.        , ...,   57.        ,\n",
       "         171.        ,    0.        ],\n",
       "       ...,\n",
       "       [  90.        ,    0.        ,    0.        , ...,   81.        ,\n",
       "        1071.        ,    1.26085034],\n",
       "       [  90.        ,    0.        ,    0.        , ...,   81.        ,\n",
       "        1155.        ,    1.2860195 ],\n",
       "       [  90.        ,    0.        ,    0.        , ...,   81.        ,\n",
       "        1239.        ,    1.2983506 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(scaler.inverse_transform(trainingdata), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diagnose',\n",
       " 'DiagnosticArtAstrup',\n",
       " 'DiagnosticBlood',\n",
       " 'DiagnosticECG',\n",
       " 'DiagnosticIC',\n",
       " 'DiagnosticLacticAcid',\n",
       " 'DiagnosticLiquor',\n",
       " 'DiagnosticOther',\n",
       " 'DiagnosticSputum',\n",
       " 'DiagnosticUrinaryCulture',\n",
       " 'DiagnosticUrinarySediment',\n",
       " 'DiagnosticXthorax',\n",
       " 'DisfuncOrg',\n",
       " 'Hypotensie',\n",
       " 'Hypoxie',\n",
       " 'InfectionSuspected',\n",
       " 'Infusion',\n",
       " 'Oligurie',\n",
       " 'SIRSCritHeartRate',\n",
       " 'SIRSCritLeucos',\n",
       " 'SIRSCritTachypnea',\n",
       " 'SIRSCritTemperature',\n",
       " 'SIRSCriteria2OrMore']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)\n",
    "dataset_manager.static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Activity', 'org:group']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.dynamic_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.static_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols for feat in range(len(feat_list)) if col in feat_list[feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['agg__Activity_Admission_IC', 'agg__Activity_Admission_NC',\n",
       "       'agg__Activity_CRP', 'agg__Activity_ER_Registration',\n",
       "       'agg__Activity_ER_Sepsis_Triage', 'agg__Activity_ER_Triage',\n",
       "       'agg__Activity_IV_Antibiotics', 'agg__Activity_IV_Liquid',\n",
       "       'agg__Activity_LacticAcid', 'agg__Activity_Leucocytes',\n",
       "       'agg__Activity_Release_A', 'agg__Activity_Release_B',\n",
       "       'agg__Activity_Release_C', 'agg__Activity_Release_D',\n",
       "       'agg__Activity_other', 'agg__org:group_A', 'agg__org:group_B',\n",
       "       'agg__org:group_C', 'agg__org:group_D', 'agg__org:group_E',\n",
       "       'agg__org:group_F', 'agg__org:group_G', 'agg__org:group_H',\n",
       "       'agg__org:group_I', 'agg__org:group_J', 'agg__org:group_K',\n",
       "       'agg__org:group_L', 'agg__org:group_M', 'agg__org:group_N',\n",
       "       'agg__org:group_O', 'agg__org:group_P', 'agg__org:group_Q',\n",
       "       'agg__org:group_R', 'agg__org:group_S', 'agg__org:group_T',\n",
       "       'agg__org:group_U', 'agg__org:group_V', 'agg__org:group_W',\n",
       "       'static__Diagnose_A', 'static__Diagnose_AA', 'static__Diagnose_AB',\n",
       "       'static__Diagnose_AC', 'static__Diagnose_AD',\n",
       "       'static__Diagnose_AE', 'static__Diagnose_B', 'static__Diagnose_BA',\n",
       "       'static__Diagnose_BB', 'static__Diagnose_BC', 'static__Diagnose_C',\n",
       "       'static__Diagnose_CA', 'static__Diagnose_CD',\n",
       "       'static__Diagnose_CE', 'static__Diagnose_D', 'static__Diagnose_DB',\n",
       "       'static__Diagnose_E', 'static__Diagnose_EA', 'static__Diagnose_EB',\n",
       "       'static__Diagnose_EC', 'static__Diagnose_ED', 'static__Diagnose_F',\n",
       "       'static__Diagnose_FA', 'static__Diagnose_FB',\n",
       "       'static__Diagnose_FD', 'static__Diagnose_FE', 'static__Diagnose_G',\n",
       "       'static__Diagnose_GA', 'static__Diagnose_GB',\n",
       "       'static__Diagnose_GC', 'static__Diagnose_GD',\n",
       "       'static__Diagnose_GE', 'static__Diagnose_H', 'static__Diagnose_HA',\n",
       "       'static__Diagnose_HB', 'static__Diagnose_HC',\n",
       "       'static__Diagnose_HE', 'static__Diagnose_I', 'static__Diagnose_IA',\n",
       "       'static__Diagnose_IB', 'static__Diagnose_IC',\n",
       "       'static__Diagnose_ID', 'static__Diagnose_J', 'static__Diagnose_JA',\n",
       "       'static__Diagnose_JC', 'static__Diagnose_JD',\n",
       "       'static__Diagnose_JE', 'static__Diagnose_K', 'static__Diagnose_KA',\n",
       "       'static__Diagnose_KB', 'static__Diagnose_KC',\n",
       "       'static__Diagnose_LA', 'static__Diagnose_LB',\n",
       "       'static__Diagnose_LC', 'static__Diagnose_LD',\n",
       "       'static__Diagnose_LE', 'static__Diagnose_M', 'static__Diagnose_MA',\n",
       "       'static__Diagnose_MC', 'static__Diagnose_MD',\n",
       "       'static__Diagnose_ME', 'static__Diagnose_N', 'static__Diagnose_NB',\n",
       "       'static__Diagnose_ND', 'static__Diagnose_O', 'static__Diagnose_OA',\n",
       "       'static__Diagnose_OB', 'static__Diagnose_OC',\n",
       "       'static__Diagnose_OE', 'static__Diagnose_P', 'static__Diagnose_PB',\n",
       "       'static__Diagnose_PC', 'static__Diagnose_PD', 'static__Diagnose_Q',\n",
       "       'static__Diagnose_QA', 'static__Diagnose_QB', 'static__Diagnose_R',\n",
       "       'static__Diagnose_RA', 'static__Diagnose_RB',\n",
       "       'static__Diagnose_RC', 'static__Diagnose_RD', 'static__Diagnose_S',\n",
       "       'static__Diagnose_SA', 'static__Diagnose_SB',\n",
       "       'static__Diagnose_SC', 'static__Diagnose_SD', 'static__Diagnose_T',\n",
       "       'static__Diagnose_TA', 'static__Diagnose_TB', 'static__Diagnose_U',\n",
       "       'static__Diagnose_UA', 'static__Diagnose_UC',\n",
       "       'static__Diagnose_VA', 'static__Diagnose_VB',\n",
       "       'static__Diagnose_VD', 'static__Diagnose_W', 'static__Diagnose_WA',\n",
       "       'static__Diagnose_WC', 'static__Diagnose_WD',\n",
       "       'static__Diagnose_XA', 'static__Diagnose_XB',\n",
       "       'static__Diagnose_XD', 'static__Diagnose_Y', 'static__Diagnose_YA',\n",
       "       'static__Diagnose_YD', 'static__Diagnose_Z', 'static__Diagnose_ZA',\n",
       "       'static__Diagnose_ZB', 'static__Diagnose_ZC',\n",
       "       'static__Diagnose_ZD', 'static__Diagnose_missing',\n",
       "       'static__Diagnose_other', 'static__DiagnosticArtAstrup_False',\n",
       "       'static__DiagnosticArtAstrup_True',\n",
       "       'static__DiagnosticArtAstrup_missing',\n",
       "       'static__DiagnosticBlood_False', 'static__DiagnosticBlood_True',\n",
       "       'static__DiagnosticBlood_missing', 'static__DiagnosticECG_False',\n",
       "       'static__DiagnosticECG_True', 'static__DiagnosticECG_missing',\n",
       "       'static__DiagnosticIC_False', 'static__DiagnosticIC_True',\n",
       "       'static__DiagnosticIC_missing',\n",
       "       'static__DiagnosticLacticAcid_False',\n",
       "       'static__DiagnosticLacticAcid_True',\n",
       "       'static__DiagnosticLacticAcid_missing',\n",
       "       'static__DiagnosticLiquor_False', 'static__DiagnosticLiquor_True',\n",
       "       'static__DiagnosticLiquor_missing',\n",
       "       'static__DiagnosticOther_False', 'static__DiagnosticOther_True',\n",
       "       'static__DiagnosticOther_missing',\n",
       "       'static__DiagnosticSputum_False', 'static__DiagnosticSputum_True',\n",
       "       'static__DiagnosticSputum_missing',\n",
       "       'static__DiagnosticUrinaryCulture_False',\n",
       "       'static__DiagnosticUrinaryCulture_True',\n",
       "       'static__DiagnosticUrinaryCulture_missing',\n",
       "       'static__DiagnosticUrinarySediment_False',\n",
       "       'static__DiagnosticUrinarySediment_True',\n",
       "       'static__DiagnosticUrinarySediment_missing',\n",
       "       'static__DiagnosticXthorax_False',\n",
       "       'static__DiagnosticXthorax_True',\n",
       "       'static__DiagnosticXthorax_missing', 'static__DisfuncOrg_False',\n",
       "       'static__DisfuncOrg_True', 'static__DisfuncOrg_missing',\n",
       "       'static__Hypotensie_False', 'static__Hypotensie_True',\n",
       "       'static__Hypotensie_missing', 'static__Hypoxie_False',\n",
       "       'static__Hypoxie_True', 'static__Hypoxie_missing',\n",
       "       'static__InfectionSuspected_False',\n",
       "       'static__InfectionSuspected_True',\n",
       "       'static__InfectionSuspected_missing', 'static__Infusion_False',\n",
       "       'static__Infusion_True', 'static__Infusion_missing',\n",
       "       'static__Oligurie_False', 'static__Oligurie_True',\n",
       "       'static__Oligurie_missing', 'static__SIRSCritHeartRate_False',\n",
       "       'static__SIRSCritHeartRate_True',\n",
       "       'static__SIRSCritHeartRate_missing',\n",
       "       'static__SIRSCritLeucos_False', 'static__SIRSCritLeucos_True',\n",
       "       'static__SIRSCritLeucos_missing',\n",
       "       'static__SIRSCritTachypnea_False',\n",
       "       'static__SIRSCritTachypnea_True',\n",
       "       'static__SIRSCritTachypnea_missing',\n",
       "       'static__SIRSCritTemperature_False',\n",
       "       'static__SIRSCritTemperature_True',\n",
       "       'static__SIRSCritTemperature_missing',\n",
       "       'static__SIRSCriteria2OrMore_False',\n",
       "       'static__SIRSCriteria2OrMore_True',\n",
       "       'static__SIRSCriteria2OrMore_missing'], dtype='<U41')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(feat_list)[cats]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
