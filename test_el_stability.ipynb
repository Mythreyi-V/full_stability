{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show acv-exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mythreyi/.local/lib/python3.9/site-packages/xgboost/compat.py:85: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-08-15 10:52:18.309486: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-15 10:52:18.309548: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#from acv_explainers import ACXplainer\n",
    "from learning import *\n",
    "import pyAgrum\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        elif type(shap_explainer) == shap.explainers._permutation.Permutation:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1), max_evals = 2*length+1).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))[0]\n",
    "            #print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[i]\n",
    "            abs_val = abs(shap_values[i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] >= q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter):\n",
    "    instance = instance.reshape(1, -1)\n",
    "    y = cls.predict(instance)\n",
    "    \n",
    "    t=np.var(y_train)\n",
    "\n",
    "    feats = []\n",
    "    feat_imp = []\n",
    "\n",
    "    for i in range(exp_iter):\n",
    "        sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train,\n",
    "                                                                                 t=t, pi_level=0.8)\n",
    "        clean_expl = sufficient_expl.copy()\n",
    "        clean_expl = clean_expl[0]\n",
    "        clean_expl = [sublist for sublist in clean_expl if sum(n<0 for n in sublist)==0 ]\n",
    "\n",
    "        clean_sdp = sdp_expl[0].copy()\n",
    "        clean_sdp = [sdp for sdp in clean_sdp if sdp > 0]\n",
    "        \n",
    "        lximp = explainer.compute_local_sdp(X_train.shape[1], clean_expl)\n",
    "        feat_imp.append(lximp)\n",
    "        \n",
    "        if len(clean_expl)==0 or len(clean_expl[0])==0:\n",
    "            print(\"No explamation meets pi level\")\n",
    "        else:\n",
    "            lens = [len(i) for i in clean_expl]\n",
    "            me_loc = [i for i in range(len(lens)) if lens[i]==min(lens)]\n",
    "            mse_loc = np.argmax(np.array(clean_sdp)[me_loc])\n",
    "            mse = np.array(clean_expl)[me_loc][mse_loc]\n",
    "            feats.extend(mse)\n",
    "\n",
    "    if len(feats)==0:\n",
    "        feat_pos = []\n",
    "    else:\n",
    "        feat_pos = set(feats)\n",
    "    \n",
    "      \n",
    "    feat_imp = np.mean(feat_imp, axis=0)\n",
    "    \n",
    "    return feat_imp, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list, percentile):\n",
    "    label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "    feat_pos = []\n",
    "    lkhoods = []\n",
    "    \n",
    "    save_to = os.path.join(PATH, dataset, cls_method, method_name)+\"/\"\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        [bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                           None, scaler, cls, save_to, dataset, show_in_notebook = False,\n",
    "                                                           samples=round(len(feat_list)*2))\n",
    "        \n",
    "        ie = pyAgrum.LazyPropagation(bn)\n",
    "        result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "        if len(result_posterior.shape)==1:\n",
    "            result_proba = result_posterior.values[0]\n",
    "        else:\n",
    "            result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]        \n",
    "        row = instance['original_vector']\n",
    "        #print(row)\n",
    "\n",
    "        likelihood = [0]*len(feat_list)\n",
    "\n",
    "        for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "\n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "\n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "            \n",
    "            result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "            if len(result_posterior.shape)==1:\n",
    "                new_proba = result_posterior.values[0]\n",
    "            else:\n",
    "                new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "            #print(result_proba, new_proba)\n",
    "            proba_change = result_proba-new_proba\n",
    "            likelihood[j] = abs(proba_change)\n",
    "\n",
    "        lkhoods.append(likelihood)\n",
    "        \n",
    "    min_coef = min( np.mean(lkhoods, axis=0))\n",
    "    max_coef = max( np.mean(lkhoods, axis=0))\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "\n",
    "    #If fixing all features produces the same result for the class,\n",
    "    #return all features\n",
    "    if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "        feat_pos.extend(range(len(feat_list)))\n",
    "    else:\n",
    "        feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "    feat_pos = set(feat_pos)\n",
    "        \n",
    "    return np.mean(lkhoods, axis=0), feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpic2011_f1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2011\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"logit\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "xai_method = \"LIME\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"],\n",
    "    \"bpic2011\": [\"bpic2011_f1\"],\n",
    "    \"hospital\": [\"hospital_billing_2\"],\n",
    "    \"traffic\": [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in datasets:\n",
    "\n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = max_prefix\n",
    "\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "#     #data = dataset_manager.read_dataset()\n",
    "\n",
    "#     all_pipelines = []\n",
    "#     all_cls = []\n",
    "#     all_encoders = []\n",
    "#     all_scalers = []\n",
    "#     all_train = []\n",
    "#     all_samples = []\n",
    "#     all_results = []\n",
    "\n",
    "#     dt_val_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.concat([dt_train_prefixes, dt_val_prefixes])\n",
    "#     dt_train_prefixes = dataset_manager.generate_prefix_data(dt_train_prefixes, min_prefix_length, max_prefix_length)\n",
    "\n",
    "#     if bucket_method == \"state\":\n",
    "#         bucket_encoding = \"last\"\n",
    "#     else:\n",
    "#         bucket_encoding = \"agg\"\n",
    "    \n",
    "#     bucketer_args = {'encoding_method':bucket_encoding,\n",
    "#                      'case_id_col':dataset_manager.case_id_col, \n",
    "#                      'cat_cols':[dataset_manager.activity_col], \n",
    "#                      'num_cols':[], \n",
    "#                      'random_state':random_state}\n",
    "\n",
    "#     bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "#     bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "    \n",
    "#     for ii in range(n_iter):\n",
    "#         num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "#         for bucket in tqdm_notebook(range(num_buckets)):\n",
    "#             bucketID = bucket+1\n",
    "#             print ('Bucket', bucketID)\n",
    "\n",
    "#             #import everything needed to sort and predict\n",
    "#             pipeline_path = os.path.join(PATH, \"%s/%s/%s/pipelines/pipeline_bucket_%s.joblib\" % \n",
    "#                                          (dataset_ref, cls_method, method_name, bucketID))\n",
    "#             pipeline = joblib.load(pipeline_path)\n",
    "#             feature_combiner = pipeline['encoder']\n",
    "#             if 'scaler' in pipeline.named_steps:\n",
    "#                 scaler = pipeline['scaler']\n",
    "#             else:\n",
    "#                 scaler = None\n",
    "#             cls = pipeline['cls']\n",
    "            \n",
    "#             all_cls.append(cls)\n",
    "#             all_encoders.append(feature_combiner)\n",
    "#             all_scalers.append(scaler)\n",
    "#             all_pipelines.append(pipeline)\n",
    "\n",
    "#             #find relevant samples for bucket\n",
    "#             bucket_sample = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             results_template = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID)))\n",
    "    \n",
    "#             if scaler != None:\n",
    "#                 bucket_sample = scaler.transform(bucket_sample)\n",
    "#             bucket_results = results_template\n",
    "            \n",
    "#             feat_names = feature_combiner.get_feature_names()\n",
    "#             feat_list = [feat.replace(\" \", \"_\") for feat in feat_names]\n",
    "            \n",
    "#             all_samples.append(bucket_sample)\n",
    "#             all_results.append(bucket_results)\n",
    "            \n",
    "#             #import training data for bucket\n",
    "#             train_data = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "#                                                           (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             if scaler != None:\n",
    "#                 train_data = scaler.transform(train_data)\n",
    "            \n",
    "#             all_train.append(train_data)\n",
    "            \n",
    "#             #get target values for bucket\n",
    "#             train_prefixes = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_eval(instance):\n",
    "    #instance_no += 1    \n",
    "    #print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "    #if cls_method == \"xgboost\":\n",
    "    instance = instance.reshape(1, -1)\n",
    "    pred = cls.predict(instance)\n",
    "\n",
    "    #Get Tree SHAP explanations for instance\n",
    "    exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "    feat_pres = []\n",
    "    feat_weights = []\n",
    "\n",
    "    for iteration in rel_exp:\n",
    "        #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "        presence_list = [0]*len(feat_list)\n",
    "\n",
    "        for each in feat_list:\n",
    "            list_idx = feat_list.index(each)\n",
    "\n",
    "            for explanation in iteration:\n",
    "                if each in explanation[0]:\n",
    "                    presence_list[list_idx] = 1\n",
    "\n",
    "        feat_pres.append(presence_list)\n",
    "\n",
    "    for iteration in exp:\n",
    "        #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "        weights = [0]*len(feat_list)\n",
    "\n",
    "        for each in feat_list:\n",
    "            list_idx = feat_list.index(each)\n",
    "\n",
    "            for explanation in iteration:\n",
    "                if each in explanation[0]:\n",
    "\n",
    "                    weights[list_idx] = explanation[1]\n",
    "        feat_weights.append(weights)\n",
    "\n",
    "    stability = st.getStability(feat_pres)\n",
    "    #print (\"Stability:\", round(stability,2))\n",
    "    #subset_stability.append(stability)\n",
    "\n",
    "    rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "    avg_dispersal = 1-np.mean(rel_var)\n",
    "    #print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "    #weight_stability.append(avg_dispersal)\n",
    "    adj_dispersal = 1-np.mean(second_var)\n",
    "    #print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "    #adjusted_weight_stability.append(adj_dispersal)\n",
    "    \n",
    "    return stability, avg_dispersal, adj_dispersal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "if xai_method==\"SHAP\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                        (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in tqdm_notebook(range(num_buckets)):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "                \n",
    "                #import everything needed to sort and predict\n",
    "                pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                             (bucketID))\n",
    "                pipeline = joblib.load(pipeline_path)\n",
    "                feature_combiner = pipeline['encoder']\n",
    "                if 'scaler' in pipeline.named_steps:\n",
    "                    scaler = pipeline['scaler']\n",
    "                else:\n",
    "                    scaler = None\n",
    "                cls = pipeline['cls']\n",
    "                \n",
    "                #import training data for bucket\n",
    "                trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                if scaler != None:\n",
    "                    trainingdata = scaler.transform(trainingdata)\n",
    "                    \n",
    "                #find relevant samples for bucket\n",
    "                sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "                \n",
    "                if scaler != None:\n",
    "                    sample_instances = scaler.transform(sample_instances)\n",
    "                \n",
    "                #create explanation mechanism\n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    shap_explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "                print(type(shap_explainer))\n",
    "                \n",
    "                #Identify feature names\n",
    "                feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "                \n",
    "                subset_stability = []\n",
    "                weight_stability = []\n",
    "                adjusted_weight_stability = []\n",
    "                    \n",
    "                #explain the chosen instances and find the stability score\n",
    "                pool = mp.Pool(mp.cpu_count())\n",
    "                start = time.time()\n",
    "                stability, avg_dispersal, adj_dispersal = zip(*pool.map(shap_eval, [instance for instance in sample_instances]))\n",
    "                print(time.time()-start, \"seconds\")\n",
    "                    \n",
    "                subset_stability = list(stability)\n",
    "                weight_stability = list(avg_dispersal)\n",
    "                adjusted_weight_stability = list(adj_dispersal)\n",
    "                    \n",
    "                results[\"SHAP Subset Stability\"] = subset_stability\n",
    "                results[\"SHAP Weight Stability\"] = weight_stability\n",
    "                results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "#                 results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "#                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "                all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_eval(instance):\n",
    "    #Get lime explanations for instance\n",
    "    feat_pres = []\n",
    "    feat_weights = []\n",
    "\n",
    "    for iteration in list(range(exp_iter)):\n",
    "\n",
    "        lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                              max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "        all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "        bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "        q1_min = bins[-2]\n",
    "\n",
    "        presence_list = [0]*len(feat_list)\n",
    "        weights = [0]*len(feat_list)\n",
    "\n",
    "        for each in feat_list:\n",
    "            list_idx = feat_list.index(each)\n",
    "            #print (\"Feature\", list_idx)\n",
    "            for explanation in lime_exp.as_list():\n",
    "                if each in explanation[0]:\n",
    "                    if explanation[1] >= q1_min:\n",
    "                        presence_list[list_idx] = 1\n",
    "                    weights[list_idx] = explanation[1]\n",
    "\n",
    "        feat_pres.append(presence_list)\n",
    "        feat_weights.append(weights)\n",
    "\n",
    "    stability = st.getStability(feat_pres)\n",
    "#     print (\"Stability:\", round(stability,2))\n",
    "#     subset_stability.append(stability)\n",
    "\n",
    "    rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "    avg_dispersal = 1-np.mean(rel_var)\n",
    "#     print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "#     weight_stability.append(avg_dispersal)\n",
    "    adj_dispersal = 1-np.mean(second_var)\n",
    "#     print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "#     adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "    return stability, avg_dispersal, adj_dispersal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating LIME\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b667801a99734e2f9ebde497e4a8d4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "235.59897780418396 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (5) does not match length of index (112)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m weight_stability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(avg_dispersal)\n\u001b[1;32m     56\u001b[0m adjusted_weight_stability \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(adj_dispersal)\n\u001b[0;32m---> 58\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIME Subset Stability\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m subset_stability\n\u001b[1;32m     59\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIME Weight Stability\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weight_stability\n\u001b[1;32m     60\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIME Adjusted Weight Stability\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adjusted_weight_stability\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   3838\u001b[0m     ):\n\u001b[1;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (5) does not match length of index (112)"
     ]
    }
   ],
   "source": [
    "if xai_method==\"LIME\":\n",
    "    print(\"evaluating LIME\")\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "           #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\"%(dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\"%(dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            #explain the chosen instances and find the stability score\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            start = time.time()\n",
    "            stability, avg_dispersal, adj_dispersal = zip(*pool.map(lime_eval, [instance for instance in sample_instances[:5]]))\n",
    "            print(time.time()-start, \"seconds\")\n",
    "\n",
    "            subset_stability = list(stability)\n",
    "            weight_stability = list(avg_dispersal)\n",
    "            adjusted_weight_stability = list(adj_dispersal)\n",
    "               \n",
    "            results[\"LIME Subset Stability\"] = subset_stability\n",
    "            results[\"LIME Weight Stability\"] = weight_stability\n",
    "            results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            #results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "            #                   (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "            all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119.0902650356293 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "[lime_eval(instance) for instance in sample_instances[:5]]\n",
    "print(time.time()-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "#                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"ACV\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "            \n",
    "            acv_explainer = joblib.load(os.path.join(PATH,'%s/%s/%s/acv_surrogate/acv_explainer_bucket_%s.joblib'% \n",
    "                                                                    (dataset_ref, cls_method, method_name, bucketID)))\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(sample_instances[:1]):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_acv_features(acv_explainer, instance, cls, trainingdata, targets, 1)\n",
    "                    print(weights)\n",
    "                    print(feat_pos)\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "#             results[\"ACV Subset Stability\"] = subset_stability\n",
    "#             results[\"ACV Weight Stability\"] = weight_stability\n",
    "#             results[\"ACV Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "#             results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "#                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "#                 all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LINDA\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            test_dict = generate_local_predictions( sample_instances, results[\"Actual\"], cls, scaler, None )\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(test_dict):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)\n",
    "                    #print(weights)\n",
    "                    #print(feat_pos)\n",
    "                    \n",
    "                    feat_pos = list(feat_pos)\n",
    "                    \n",
    "                    #bins = pd.cut(weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    #q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "\n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "                \n",
    "                #print(feat_pres)\n",
    "                #print(feat_weights)\n",
    "                \n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LINDA Subset Stability\"] = subset_stability\n",
    "            results[\"LINDA Weight Stability\"] = weight_stability\n",
    "            results[\"LINDA Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "            all_results.append(results)\n",
    "            \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "feat_pos = []\n",
    "lkhoods = []\n",
    "\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)+\"/\"\n",
    "\n",
    "[bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                       None, scaler, cls, save_to, dataset_ref, show_in_notebook = False,\n",
    "                                                       samples=round(len(feat_list)*1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = pyAgrum.LazyPropagation(bn)\n",
    "result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "\n",
    "if len(result_posterior.shape)==1:\n",
    "    result_proba = result_posterior.values[0]\n",
    "else:\n",
    "    result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "\n",
    "row = instance['original_vector']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "            \n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "            \n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "                \n",
    "#                print(bins_diff)\n",
    "#                 for k in range(len(bins)):\n",
    "#                     if k!=len(bins)-1 and bins[k+1][0]-bins[k][1]!=0:\n",
    "#                         print(\"Gap between bins\")\n",
    "#                         lower = val - bins[k][1]\n",
    "#                         higher = bins[k+1][0] - val\n",
    "#                         if lower > higher:\n",
    "#                             feat_bin = str_bins[k+1]\n",
    "#                         else:\n",
    "#                             feat_bin = str_bins[k]\n",
    "#                 if feat_bin==None:\n",
    "#                     print(\"Some other issue with bins\")\n",
    "                    \n",
    "            print(val)\n",
    "            #print(row[j])\n",
    "            print(str_bins[k])\n",
    "\n",
    "            ie = pyAgrum.LazyPropagation(bn)\n",
    "            ie.setEvidence({feat_list[j]: feat_bin})\n",
    "            ie.makeInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_diff = np.array(bins) - val\n",
    "print(bins_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_diff = np.array([[6, 3, 1], [2, 5, 6], [2, 0, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance[\"original_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[0][\"scaled_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.variable(feat_list[j-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showInference(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(trainingdata, columns=feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAgrum.lib.bn2graph import BN2dot, BNinference2dot\n",
    "\n",
    "g = BNinference2dot(bn, size='\"100,10!\"', engine=ie)\n",
    "\n",
    "g.write(\"bn2graph_test.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_posterior.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = pyAgrum.BNLearner(save_to+\"feature_permutations/bpic2012/false_negatives/0_permutations.csv\")\n",
    "bn = learner.learnBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = all_samples[0]\n",
    "instance = all_samples[0][0]\n",
    "trainingdata = all_train[0]\n",
    "\n",
    "cls = all_cls[0]\n",
    "scaler = all_scalers[0]\n",
    "\n",
    "feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names)\n",
    "if scaler == None:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                                 cls.predict_proba, num_features=10, labels=[0,1], top_labels=1)\n",
    "else:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                             scale_predict_fn, num_features=max_feat, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(cls)\n",
    "explanation = shap_explainer(testingdata)\n",
    "explanation.feature_names = feat_list\n",
    "shap.plots.waterfall(explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(scaler.inverse_transform(trainingdata), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)\n",
    "dataset_manager.static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.dynamic_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.static_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols for feat in range(len(feat_list)) if col in feat_list[feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(feat_list)[cats]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
