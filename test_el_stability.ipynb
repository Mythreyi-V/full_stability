{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19604,
     "status": "ok",
     "timestamp": 1605146497296,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "681d2816-1c16-44fb-9732-544020f7a38a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54251,
     "status": "ok",
     "timestamp": 1605146531950,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "565362da-9455-4e4d-d2d2-df15f46c66c0"
   },
   "outputs": [],
   "source": [
    "#!pip install lime==0.2.0.1\n",
    "#!pip install shap==0.37.0\n",
    "#!pip install xgboost==1.0.0\n",
    "#!pip install anchor-exp==0.0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show acv-exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539331,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "#from alibi.utils.data import gen_category_map\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#from acv_explainers import ACXplainer\n",
    "from learning import *\n",
    "import pyAgrum\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 61631,
     "status": "ok",
     "timestamp": 1605146539333,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539334,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "def generate_lime_explanations(explainer,test_xi, cls, submod=False, test_all_data=None, max_feat = 10, scaler=None):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    \n",
    "#     print(type(test_xi))\n",
    "#     print(type(cls.predict_proba))\n",
    "#     print(type(max_feat))\n",
    "\n",
    "    def scale_predict_fn(X):\n",
    "        scaled_data = scaler.transform(X)\n",
    "        pred = cls.predict_proba(scaled_data)\n",
    "        return pred\n",
    "\n",
    "    def predict_fn(X):\n",
    "        #X = X.reshape(1, -1)\n",
    "        pred = cls.predict_proba(X)\n",
    "        return pred\n",
    "\n",
    "    if scaler == None:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 predict_fn, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 scale_predict_fn, num_features=max_feat, labels=[0,1])\n",
    "        \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61627,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Fikq5FrPzNd6"
   },
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    \n",
    "    feat_len = len(features)\n",
    "    weights_by_feat = []\n",
    "    \n",
    "    #Weights are sorted by iteration. Transpose list.\n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #Find mean and variance of weight for each feature\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        #Calculate relative variance, ignore features where the weight is always 0\n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        else:\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "\n",
    "            for i in range(len(z_scores)):\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 61629,
     "status": "ok",
     "timestamp": 1605146539335,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "dc4VS_V-zNd3"
   },
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, pred, top = None, scaler = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    if scaler != None:\n",
    "        row = scaler.transform(row)\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        #Generate shap values for row\n",
    "        if type(shap_explainer) == shap.explainers._tree.Tree:\n",
    "            shap_values = shap_explainer(row, check_additivity = False).values\n",
    "        elif type(shap_explainer) == shap.explainers._permutation.Permutation:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1), max_evals = 2*length+1).values\n",
    "        else:\n",
    "            shap_values = shap_explainer(row.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        #print(shap_values.shape)\n",
    "        #print(len(features))\n",
    "        if shap_values.shape == (1, len(features), 2):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if shap_values.shape == (len(features), 2):\n",
    "            shap_values = np.array([feat[pred] for feat in shap_values]).reshape(len(features))\n",
    "        elif shap_values.shape == (1, len(row)) or shap_values.shape == (len(features), 1):\n",
    "            shap_values = shap_values.reshape(len(features))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "        \n",
    "        if scaler != None:\n",
    "            #print(shap_values)\n",
    "            shap_values = scaler.inverse_transform(shap_values.reshape(1, -1))[0]\n",
    "            #print(shap_values.shape)\n",
    "        \n",
    "        #Map SHAP values to feature names\n",
    "        importances = []\n",
    "        \n",
    "        abs_values = []\n",
    "    \n",
    "        for i in range(length):\n",
    "            feat = features[i]\n",
    "            shap_val = shap_values[i]\n",
    "            abs_val = abs(shap_values[i])\n",
    "            entry = (feat, shap_val, abs_val)\n",
    "            importances.append(entry)\n",
    "            abs_values.append(abs_val)\n",
    "        \n",
    "        #Sort features by influence on result\n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        #Create list of all feature\n",
    "        exp.append(importances)\n",
    "        \n",
    "        #print(exp[0])\n",
    "        \n",
    "        #Create list of most important features\n",
    "        rel_feat = []\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            bins = pd.cut(abs_values, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "            q1_min = bins[-2]\n",
    "            rel_feat = [feat for feat in importances if feat[2] >= q1_min]\n",
    "            rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter):\n",
    "    instance = instance.reshape(1, -1)\n",
    "    y = cls.predict(instance)\n",
    "    \n",
    "    t=np.var(y_train)\n",
    "\n",
    "    feats = []\n",
    "    feat_imp = []\n",
    "\n",
    "    for i in range(exp_iter):\n",
    "        sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train,\n",
    "                                                                                 t=t, pi_level=0.8)\n",
    "        clean_expl = sufficient_expl.copy()\n",
    "        clean_expl = clean_expl[0]\n",
    "        clean_expl = [sublist for sublist in clean_expl if sum(n<0 for n in sublist)==0 ]\n",
    "\n",
    "        clean_sdp = sdp_expl[0].copy()\n",
    "        clean_sdp = [sdp for sdp in clean_sdp if sdp > 0]\n",
    "        \n",
    "        lximp = explainer.compute_local_sdp(X_train.shape[1], clean_expl)\n",
    "        feat_imp.append(lximp)\n",
    "        \n",
    "        if len(clean_expl)==0 or len(clean_expl[0])==0:\n",
    "            print(\"No explamation meets pi level\")\n",
    "        else:\n",
    "            lens = [len(i) for i in clean_expl]\n",
    "            me_loc = [i for i in range(len(lens)) if lens[i]==min(lens)]\n",
    "            mse_loc = np.argmax(np.array(clean_sdp)[me_loc])\n",
    "            mse = np.array(clean_expl)[me_loc][mse_loc]\n",
    "            feats.extend(mse)\n",
    "\n",
    "    if len(feats)==0:\n",
    "        feat_pos = []\n",
    "    else:\n",
    "        feat_pos = set(feats)\n",
    "    \n",
    "      \n",
    "    feat_imp = np.mean(feat_imp, axis=0)\n",
    "    \n",
    "    return feat_imp, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list, percentile):\n",
    "    label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "    feat_pos = []\n",
    "    lkhoods = []\n",
    "    \n",
    "    save_to = os.path.join(PATH, dataset, cls_method, method_name)+\"/\"\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        [bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                           None, scaler, cls, save_to, dataset, show_in_notebook = False,\n",
    "                                                           samples=round(len(feat_list)*2))\n",
    "        \n",
    "        ie = pyAgrum.LazyPropagation(bn)\n",
    "        result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "        if len(result_posterior.shape)==1:\n",
    "            result_proba = result_posterior.values[0]\n",
    "        else:\n",
    "            result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]        \n",
    "        row = instance['original_vector']\n",
    "        #print(row)\n",
    "\n",
    "        likelihood = [0]*len(feat_list)\n",
    "\n",
    "        for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "\n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "\n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "            \n",
    "            result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "            if len(result_posterior.shape)==1:\n",
    "                new_proba = result_posterior.values[0]\n",
    "            else:\n",
    "                new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "            #print(result_proba, new_proba)\n",
    "            proba_change = result_proba-new_proba\n",
    "            likelihood[j] = abs(proba_change)\n",
    "\n",
    "        lkhoods.append(likelihood)\n",
    "        \n",
    "    min_coef = min( np.mean(lkhoods, axis=0))\n",
    "    max_coef = max( np.mean(lkhoods, axis=0))\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "\n",
    "    #If fixing all features produces the same result for the class,\n",
    "    #return all features\n",
    "    if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "        feat_pos.extend(range(len(feat_list)))\n",
    "    else:\n",
    "        feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "    feat_pos = set(feat_pos)\n",
    "        \n",
    "    return np.mean(lkhoods, axis=0), feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61624,
     "status": "ok",
     "timestamp": 1605146539336,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK",
    "outputId": "789e193d-b6cd-4f6f-817e-17637de27f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpic2011_f1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2011\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "xai_method = \"SHAP\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"],\n",
    "    \"bpic2011\": [\"bpic2011_f1\"],\n",
    "    \"hospital\": [\"hospital_billing_2\"],\n",
    "    \"traffic\": [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_name in datasets:\n",
    "\n",
    "#     min_prefix_length = 1\n",
    "#     max_prefix_length = max_prefix\n",
    "\n",
    "#     dataset_manager = DatasetManager(dataset_name)\n",
    "#     #data = dataset_manager.read_dataset()\n",
    "\n",
    "#     all_pipelines = []\n",
    "#     all_cls = []\n",
    "#     all_encoders = []\n",
    "#     all_scalers = []\n",
    "#     all_train = []\n",
    "#     all_samples = []\n",
    "#     all_results = []\n",
    "\n",
    "#     dt_val_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" %(dataset_ref)))\n",
    "#     dt_train_prefixes = pd.concat([dt_train_prefixes, dt_val_prefixes])\n",
    "#     dt_train_prefixes = dataset_manager.generate_prefix_data(dt_train_prefixes, min_prefix_length, max_prefix_length)\n",
    "\n",
    "#     if bucket_method == \"state\":\n",
    "#         bucket_encoding = \"last\"\n",
    "#     else:\n",
    "#         bucket_encoding = \"agg\"\n",
    "    \n",
    "#     bucketer_args = {'encoding_method':bucket_encoding,\n",
    "#                      'case_id_col':dataset_manager.case_id_col, \n",
    "#                      'cat_cols':[dataset_manager.activity_col], \n",
    "#                      'num_cols':[], \n",
    "#                      'random_state':random_state}\n",
    "\n",
    "#     bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "#     bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "    \n",
    "#     for ii in range(n_iter):\n",
    "#         num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "#         for bucket in tqdm_notebook(range(num_buckets)):\n",
    "#             bucketID = bucket+1\n",
    "#             print ('Bucket', bucketID)\n",
    "\n",
    "#             #import everything needed to sort and predict\n",
    "#             pipeline_path = os.path.join(PATH, \"%s/%s/%s/pipelines/pipeline_bucket_%s.joblib\" % \n",
    "#                                          (dataset_ref, cls_method, method_name, bucketID))\n",
    "#             pipeline = joblib.load(pipeline_path)\n",
    "#             feature_combiner = pipeline['encoder']\n",
    "#             if 'scaler' in pipeline.named_steps:\n",
    "#                 scaler = pipeline['scaler']\n",
    "#             else:\n",
    "#                 scaler = None\n",
    "#             cls = pipeline['cls']\n",
    "            \n",
    "#             all_cls.append(cls)\n",
    "#             all_encoders.append(feature_combiner)\n",
    "#             all_scalers.append(scaler)\n",
    "#             all_pipelines.append(pipeline)\n",
    "\n",
    "#             #find relevant samples for bucket\n",
    "#             bucket_sample = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             results_template = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "#                                       (dataset_ref, cls_method, method_name, bucketID)))\n",
    "    \n",
    "#             if scaler != None:\n",
    "#                 bucket_sample = scaler.transform(bucket_sample)\n",
    "#             bucket_results = results_template\n",
    "            \n",
    "#             feat_names = feature_combiner.get_feature_names()\n",
    "#             feat_list = [feat.replace(\" \", \"_\") for feat in feat_names]\n",
    "            \n",
    "#             all_samples.append(bucket_sample)\n",
    "#             all_results.append(bucket_results)\n",
    "            \n",
    "#             #import training data for bucket\n",
    "#             train_data = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "#                                                           (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "#             if scaler != None:\n",
    "#                 train_data = scaler.transform(train_data)\n",
    "            \n",
    "#             all_train.append(train_data)\n",
    "            \n",
    "#             #get target values for bucket\n",
    "#             train_prefixes = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180603,
     "status": "ok",
     "timestamp": 1605146658324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "n9Ll73m1zNeF",
    "outputId": "8a950c98-fc9c-441a-bbf3-ad8279e8ef0f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a62ac63b2d4f8ea46eab3a3921d151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "<class 'shap.explainers._permutation.Permutation'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d3efd87f804f3daa8890075e728b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 of 112 .\n",
      "Stability: 0.27\n",
      "Dispersal of feature importance: -12.36\n",
      "Dispersal with no outliers: -12.36\n",
      "Testing 2 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.75\n",
      "Dispersal with no outliers: 0.75\n",
      "Testing 3 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.99\n",
      "Dispersal with no outliers: 0.99\n",
      "Testing 4 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.92\n",
      "Dispersal with no outliers: 0.92\n",
      "Testing 5 of 112 .\n",
      "Stability: 0.19\n",
      "Dispersal of feature importance: -18.92\n",
      "Dispersal with no outliers: -18.92\n",
      "Testing 6 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n",
      "Testing 7 of 112 .\n",
      "Stability: 0.83\n",
      "Dispersal of feature importance: -2.26\n",
      "Dispersal with no outliers: -2.26\n",
      "Testing 8 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.87\n",
      "Dispersal with no outliers: 0.87\n",
      "Testing 9 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.99\n",
      "Dispersal with no outliers: 0.99\n",
      "Testing 10 of 112 .\n",
      "Stability: 0.3\n",
      "Dispersal of feature importance: -5.81\n",
      "Dispersal with no outliers: -5.81\n",
      "Testing 11 of 112 .\n",
      "Stability: 0.87\n",
      "Dispersal of feature importance: -2.58\n",
      "Dispersal with no outliers: -2.58\n",
      "Testing 12 of 112 .\n",
      "Stability: 0.6\n",
      "Dispersal of feature importance: -8.61\n",
      "Dispersal with no outliers: -8.61\n",
      "Testing 13 of 112 .\n",
      "Stability: 0.91\n",
      "Dispersal of feature importance: -1.77\n",
      "Dispersal with no outliers: -1.77\n",
      "Testing 14 of 112 .\n",
      "Stability: 0.87\n",
      "Dispersal of feature importance: -1.45\n",
      "Dispersal with no outliers: -1.45\n",
      "Testing 15 of 112 .\n",
      "Stability: 0.9\n",
      "Dispersal of feature importance: -3.57\n",
      "Dispersal with no outliers: -3.57\n",
      "Testing 16 of 112 .\n",
      "Stability: 0.87\n",
      "Dispersal of feature importance: -0.52\n",
      "Dispersal with no outliers: -0.52\n",
      "Testing 17 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.99\n",
      "Dispersal with no outliers: 0.99\n",
      "Testing 18 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.79\n",
      "Dispersal with no outliers: 0.79\n",
      "Testing 19 of 112 .\n",
      "Stability: 0.4\n",
      "Dispersal of feature importance: -6.16\n",
      "Dispersal with no outliers: -6.16\n",
      "Testing 20 of 112 .\n",
      "Stability: 0.95\n",
      "Dispersal of feature importance: -1.47\n",
      "Dispersal with no outliers: -1.47\n",
      "Testing 21 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -5.08\n",
      "Dispersal with no outliers: -5.08\n",
      "Testing 22 of 112 .\n",
      "Stability: 0.84\n",
      "Dispersal of feature importance: -0.95\n",
      "Dispersal with no outliers: -0.95\n",
      "Testing 23 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.85\n",
      "Dispersal with no outliers: 0.85\n",
      "Testing 24 of 112 .\n",
      "Stability: 0.28\n",
      "Dispersal of feature importance: -12.69\n",
      "Dispersal with no outliers: -12.69\n",
      "Testing 25 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.95\n",
      "Dispersal with no outliers: 0.95\n",
      "Testing 26 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.87\n",
      "Dispersal with no outliers: 0.87\n",
      "Testing 27 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.99\n",
      "Dispersal with no outliers: 0.99\n",
      "Testing 28 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.35\n",
      "Dispersal with no outliers: 0.35\n",
      "Testing 29 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -1.39\n",
      "Dispersal with no outliers: -1.39\n",
      "Testing 30 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -9.34\n",
      "Dispersal with no outliers: -9.34\n",
      "Testing 31 of 112 .\n",
      "Stability: 0.19\n",
      "Dispersal of feature importance: -11.76\n",
      "Dispersal with no outliers: -11.76\n",
      "Testing 32 of 112 .\n",
      "Stability: 0.4\n",
      "Dispersal of feature importance: -3.63\n",
      "Dispersal with no outliers: -3.63\n",
      "Testing 33 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.04\n",
      "Dispersal with no outliers: 0.04\n",
      "Testing 34 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.99\n",
      "Dispersal with no outliers: 0.99\n",
      "Testing 35 of 112 .\n",
      "Stability: 0.25\n",
      "Dispersal of feature importance: -9.73\n",
      "Dispersal with no outliers: -9.73\n",
      "Testing 36 of 112 .\n",
      "Stability: 0.28\n",
      "Dispersal of feature importance: -17.88\n",
      "Dispersal with no outliers: -17.88\n",
      "Testing 37 of 112 .\n",
      "Stability: 0.05\n",
      "Dispersal of feature importance: -9.29\n",
      "Dispersal with no outliers: -9.29\n",
      "Testing 38 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -5.53\n",
      "Dispersal with no outliers: -5.53\n",
      "Testing 39 of 112 .\n",
      "Stability: 0.26\n",
      "Dispersal of feature importance: -4.99\n",
      "Dispersal with no outliers: -4.99\n",
      "Testing 40 of 112 .\n",
      "Stability: 0.91\n",
      "Dispersal of feature importance: -1.71\n",
      "Dispersal with no outliers: -1.71\n",
      "Testing 41 of 112 .\n",
      "Stability: 0.87\n",
      "Dispersal of feature importance: -2.43\n",
      "Dispersal with no outliers: -2.43\n",
      "Testing 42 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -3.84\n",
      "Dispersal with no outliers: -3.84\n",
      "Testing 43 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.15\n",
      "Dispersal with no outliers: 0.15\n",
      "Testing 44 of 112 .\n",
      "Stability: 0.46\n",
      "Dispersal of feature importance: -9.24\n",
      "Dispersal with no outliers: -9.24\n",
      "Testing 45 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.92\n",
      "Dispersal with no outliers: 0.92\n",
      "Testing 46 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -12.87\n",
      "Dispersal with no outliers: -12.87\n",
      "Testing 47 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.45\n",
      "Dispersal with no outliers: 0.45\n",
      "Testing 48 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -11.7\n",
      "Dispersal with no outliers: -11.7\n",
      "Testing 49 of 112 .\n",
      "Stability: 0.62\n",
      "Dispersal of feature importance: -5.23\n",
      "Dispersal with no outliers: -5.23\n",
      "Testing 50 of 112 .\n",
      "Stability: 0.95\n",
      "Dispersal of feature importance: -1.12\n",
      "Dispersal with no outliers: -1.12\n",
      "Testing 51 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.21\n",
      "Dispersal with no outliers: 0.21\n",
      "Testing 52 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -7.63\n",
      "Dispersal with no outliers: -7.63\n",
      "Testing 53 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.26\n",
      "Dispersal with no outliers: 0.26\n",
      "Testing 54 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.98\n",
      "Dispersal with no outliers: 0.98\n",
      "Testing 55 of 112 .\n",
      "Stability: 0.25\n",
      "Dispersal of feature importance: -19.98\n",
      "Dispersal with no outliers: -19.98\n",
      "Testing 56 of 112 .\n",
      "Stability: 0.63\n",
      "Dispersal of feature importance: -5.0\n",
      "Dispersal with no outliers: -5.0\n",
      "Testing 57 of 112 .\n",
      "Stability: 0.6\n",
      "Dispersal of feature importance: -4.25\n",
      "Dispersal with no outliers: -4.25\n",
      "Testing 58 of 112 .\n",
      "Stability: 0.91\n",
      "Dispersal of feature importance: -2.29\n",
      "Dispersal with no outliers: -2.29\n",
      "Testing 59 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.91\n",
      "Dispersal with no outliers: 0.91\n",
      "Testing 60 of 112 .\n",
      "Stability: 0.63\n",
      "Dispersal of feature importance: -2.9\n",
      "Dispersal with no outliers: -2.9\n",
      "Testing 61 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.92\n",
      "Dispersal with no outliers: 0.92\n",
      "Testing 62 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.79\n",
      "Dispersal with no outliers: 0.79\n",
      "Testing 63 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -5.9\n",
      "Dispersal with no outliers: -5.9\n",
      "Testing 64 of 112 .\n",
      "Stability: 0.69\n",
      "Dispersal of feature importance: -0.9\n",
      "Dispersal with no outliers: -0.9\n",
      "Testing 65 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.93\n",
      "Dispersal with no outliers: 0.93\n",
      "Testing 66 of 112 .\n",
      "Stability: 0.66\n",
      "Dispersal of feature importance: -3.67\n",
      "Dispersal with no outliers: -3.67\n",
      "Testing 67 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -10.51\n",
      "Dispersal with no outliers: -10.51\n",
      "Testing 68 of 112 .\n",
      "Stability: 0.37\n",
      "Dispersal of feature importance: -4.56\n",
      "Dispersal with no outliers: -4.56\n",
      "Testing 69 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.8\n",
      "Dispersal with no outliers: 0.8\n",
      "Testing 70 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.95\n",
      "Dispersal with no outliers: 0.95\n",
      "Testing 71 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.81\n",
      "Dispersal with no outliers: 0.81\n",
      "Testing 72 of 112 .\n",
      "Stability: 0.95\n",
      "Dispersal of feature importance: -1.36\n",
      "Dispersal with no outliers: -1.36\n",
      "Testing 73 of 112 .\n",
      "Stability: 0.44\n",
      "Dispersal of feature importance: -6.41\n",
      "Dispersal with no outliers: -6.41\n",
      "Testing 74 of 112 .\n",
      "Stability: 0.57\n",
      "Dispersal of feature importance: -7.04\n",
      "Dispersal with no outliers: -7.04\n",
      "Testing 75 of 112 .\n",
      "Stability: 0.29\n",
      "Dispersal of feature importance: -7.88\n",
      "Dispersal with no outliers: -7.88\n",
      "Testing 76 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -8.81\n",
      "Dispersal with no outliers: -8.81\n",
      "Testing 77 of 112 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.26\n",
      "Dispersal with no outliers: 0.26\n",
      "Testing 78 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.89\n",
      "Dispersal with no outliers: 0.89\n",
      "Testing 79 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -0.6\n",
      "Dispersal with no outliers: -0.6\n",
      "Testing 80 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.95\n",
      "Dispersal with no outliers: 0.95\n",
      "Testing 81 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.84\n",
      "Dispersal with no outliers: 0.84\n",
      "Testing 82 of 112 .\n",
      "Stability: 0.91\n",
      "Dispersal of feature importance: -2.07\n",
      "Dispersal with no outliers: -2.07\n",
      "Testing 83 of 112 .\n",
      "Stability: 0.87\n",
      "Dispersal of feature importance: -2.24\n",
      "Dispersal with no outliers: -2.24\n",
      "Testing 84 of 112 .\n",
      "Stability: 0.33\n",
      "Dispersal of feature importance: -7.46\n",
      "Dispersal with no outliers: -7.46\n",
      "Testing 85 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.79\n",
      "Dispersal with no outliers: 0.79\n",
      "Testing 86 of 112 .\n",
      "Stability: 0.66\n",
      "Dispersal of feature importance: -8.92\n",
      "Dispersal with no outliers: -8.92\n",
      "Testing 87 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.95\n",
      "Dispersal with no outliers: 0.95\n",
      "Testing 88 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -0.39\n",
      "Dispersal with no outliers: -0.39\n",
      "Testing 89 of 112 .\n",
      "Stability: 0.62\n",
      "Dispersal of feature importance: -8.26\n",
      "Dispersal with no outliers: -8.26\n",
      "Testing 90 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -9.44\n",
      "Dispersal with no outliers: -9.44\n",
      "Testing 91 of 112 .\n",
      "Stability: 0.95\n",
      "Dispersal of feature importance: -0.57\n",
      "Dispersal with no outliers: -0.57\n",
      "Testing 92 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.43\n",
      "Dispersal with no outliers: 0.43\n",
      "Testing 93 of 112 .\n",
      "Stability: 0.95\n",
      "Dispersal of feature importance: -0.91\n",
      "Dispersal with no outliers: -0.91\n",
      "Testing 94 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.84\n",
      "Dispersal with no outliers: 0.84\n",
      "Testing 95 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.85\n",
      "Dispersal with no outliers: 0.85\n",
      "Testing 96 of 112 .\n",
      "Stability: 0.85\n",
      "Dispersal of feature importance: -2.3\n",
      "Dispersal with no outliers: -2.3\n",
      "Testing 97 of 112 .\n",
      "Stability: 0.59\n",
      "Dispersal of feature importance: -5.35\n",
      "Dispersal with no outliers: -5.35\n",
      "Testing 98 of 112 .\n",
      "Stability: 0.4\n",
      "Dispersal of feature importance: -9.75\n",
      "Dispersal with no outliers: -9.75\n",
      "Testing 99 of 112 .\n",
      "Stability: 0.57\n",
      "Dispersal of feature importance: -5.32\n",
      "Dispersal with no outliers: -5.32\n",
      "Testing 100 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.22\n",
      "Dispersal with no outliers: 0.22\n",
      "Testing 101 of 112 .\n",
      "Stability: 0.66\n",
      "Dispersal of feature importance: -4.42\n",
      "Dispersal with no outliers: -4.42\n",
      "Testing 102 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.97\n",
      "Dispersal with no outliers: 0.97\n",
      "Testing 103 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -7.55\n",
      "Dispersal with no outliers: -7.55\n",
      "Testing 104 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -1.36\n",
      "Dispersal with no outliers: -1.36\n",
      "Testing 105 of 112 .\n",
      "Stability: 0.8\n",
      "Dispersal of feature importance: -3.2\n",
      "Dispersal with no outliers: -3.2\n",
      "Testing 106 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.58\n",
      "Dispersal with no outliers: 0.58\n",
      "Testing 107 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: -0.67\n",
      "Dispersal with no outliers: -0.67\n",
      "Testing 108 of 112 .\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.92\n",
      "Dispersal with no outliers: 0.92\n",
      "Testing 109 of 112 .\n",
      "Stability: 0.4\n",
      "Dispersal of feature importance: -5.17\n",
      "Dispersal with no outliers: -5.17\n",
      "Testing 110 of 112 .\n",
      "Stability: 0.62\n",
      "Dispersal of feature importance: -6.62\n",
      "Dispersal with no outliers: -6.62\n",
      "Testing 111 of 112 .\n",
      "Stability: 0.7\n",
      "Dispersal of feature importance: -7.02\n",
      "Dispersal with no outliers: -7.02\n",
      "Testing 112 of 112 .\n",
      "Stability: 0.46\n",
      "Dispersal of feature importance: -4.97\n",
      "Dispersal with no outliers: -4.97\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "if xai_method==\"SHAP\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                        (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in tqdm_notebook(range(num_buckets)):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "                \n",
    "                #import everything needed to sort and predict\n",
    "                pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                             (bucketID))\n",
    "                pipeline = joblib.load(pipeline_path)\n",
    "                feature_combiner = pipeline['encoder']\n",
    "                if 'scaler' in pipeline.named_steps:\n",
    "                    scaler = pipeline['scaler']\n",
    "                else:\n",
    "                    scaler = None\n",
    "                cls = pipeline['cls']\n",
    "                \n",
    "                #import training data for bucket\n",
    "                trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                              (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                if scaler != None:\n",
    "                    trainingdata = scaler.transform(trainingdata)\n",
    "                    \n",
    "                #find relevant samples for bucket\n",
    "                sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "                results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                          (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "                \n",
    "                if scaler != None:\n",
    "                    sample_instances = scaler.transform(sample_instances)\n",
    "                \n",
    "                #create explanation mechanism\n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    shap_explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    shap_explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    shap_explainer = shap.Explainer(cls, trainingdata)\n",
    "                print(type(shap_explainer))\n",
    "                \n",
    "                #Identify feature names\n",
    "                feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "                \n",
    "                subset_stability = []\n",
    "                weight_stability = []\n",
    "                adjusted_weight_stability = []\n",
    "                    \n",
    "                #explain the chosen instances and find the stability score\n",
    "                instance_no = 0\n",
    "                for instance in tqdm_notebook(sample_instances):\n",
    "                    instance_no += 1    \n",
    "                    print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "                    \n",
    "                    #if cls_method == \"xgboost\":\n",
    "                    instance = instance.reshape(1, -1)\n",
    "                    pred = cls.predict(instance)\n",
    "\n",
    "                    #Get Tree SHAP explanations for instance\n",
    "                    exp, rel_exp = create_samples(shap_explainer, exp_iter, instance, feat_list, pred, scaler = scaler)\n",
    "\n",
    "                    feat_pres = []\n",
    "                    feat_weights = []\n",
    "\n",
    "                    for iteration in rel_exp:\n",
    "                        #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "\n",
    "                        presence_list = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "                                    presence_list[list_idx] = 1\n",
    "\n",
    "                        feat_pres.append(presence_list)\n",
    "\n",
    "                    for iteration in exp:\n",
    "                        #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "\n",
    "                        weights = [0]*len(feat_list)\n",
    "\n",
    "                        for each in feat_list:\n",
    "                            list_idx = feat_list.index(each)\n",
    "\n",
    "                            for explanation in iteration:\n",
    "                                if each in explanation[0]:\n",
    "\n",
    "                                    weights[list_idx] = explanation[1]\n",
    "                        feat_weights.append(weights)\n",
    "\n",
    "                    stability = st.getStability(feat_pres)\n",
    "                    print (\"Stability:\", round(stability,2))\n",
    "                    subset_stability.append(stability)\n",
    "\n",
    "                    rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                    avg_dispersal = 1-np.mean(rel_var)\n",
    "                    print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                    weight_stability.append(avg_dispersal)\n",
    "                    adj_dispersal = 1-np.mean(second_var)\n",
    "                    print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                    adjusted_weight_stability.append(adj_dispersal)\n",
    "                    \n",
    "                results[\"SHAP Subset Stability\"] = subset_stability\n",
    "                results[\"SHAP Weight Stability\"] = weight_stability\n",
    "                results[\"SHAP Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "                results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "                all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LIME\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(9, num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "           #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(sample_instances):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "\n",
    "                    lime_exp = generate_lime_explanations(lime_explainer, instance, cls,\n",
    "                                                          max_feat = len(feat_list), scaler = scaler)\n",
    "\n",
    "                    all_weights = [exp[1] for exp in lime_exp.as_list()]\n",
    "                    bins = pd.cut(all_weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = [0]*len(feat_list)\n",
    "                    weights = [0]*len(feat_list)\n",
    "\n",
    "                    for each in feat_list:\n",
    "                        list_idx = feat_list.index(each)\n",
    "                        #print (\"Feature\", list_idx)\n",
    "                        for explanation in lime_exp.as_list():\n",
    "                            if each in explanation[0]:\n",
    "                                if explanation[1] >= q1_min:\n",
    "                                    presence_list[list_idx] = 1\n",
    "                                weights[list_idx] = explanation[1]\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LIME Subset Stability\"] = subset_stability\n",
    "            results[\"LIME Weight Stability\"] = weight_stability\n",
    "            results[\"LIME Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)\n",
    "                \n",
    "            all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "#                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10962196,
     "status": "ok",
     "timestamp": 1605157439921,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "YDm16f75zNeK",
    "outputId": "13fc0d73-546e-4d91-8926-899df530d41c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"ACV\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "            \n",
    "            acv_explainer = joblib.load(os.path.join(PATH,'%s/%s/%s/acv_surrogate/acv_explainer_bucket_%s.joblib'% \n",
    "                                                                    (dataset_ref, cls_method, method_name, bucketID)))\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(sample_instances[:1]):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_acv_features(acv_explainer, instance, cls, trainingdata, targets, 1)\n",
    "                    print(weights)\n",
    "                    print(feat_pos)\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "\n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "#             results[\"ACV Subset Stability\"] = subset_stability\n",
    "#             results[\"ACV Weight Stability\"] = weight_stability\n",
    "#             results[\"ACV Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "#             results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "#                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "#                 all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if xai_method==\"LINDA\":\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "            \n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "\n",
    "            test_dict = generate_local_predictions( sample_instances, results[\"Actual\"], cls, scaler, None )\n",
    "\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "#             class_names = [\"Negative\", \"Positive\"]\n",
    "            \n",
    "#             cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "#                     for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "\n",
    "            subset_stability = []\n",
    "            weight_stability = []\n",
    "            adjusted_weight_stability = []\n",
    "\n",
    "            #create explainer now that can be passed later\n",
    "#             lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "#                                   feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            \n",
    "            instance_no = 0\n",
    "            print(len(sample_instances))\n",
    "            #explain the chosen instances and find the stability score\n",
    "            for instance in tqdm_notebook(test_dict):\n",
    "                instance_no += 1\n",
    "\n",
    "                print(\"Testing\", instance_no, \"of\", len(sample_instances), \".\")\n",
    "\n",
    "                #Get lime explanations for instance\n",
    "                feat_pres = []\n",
    "                feat_weights = []\n",
    "                \n",
    "               \n",
    "                \n",
    "                for iteration in list(range(exp_iter)):\n",
    "                    weights, feat_pos = get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)\n",
    "                    #print(weights)\n",
    "                    #print(feat_pos)\n",
    "                    \n",
    "                    feat_pos = list(feat_pos)\n",
    "                    \n",
    "                    #bins = pd.cut(weights, 4, duplicates = \"drop\", retbins = True)[-1]\n",
    "                    #q1_min = bins[-2]\n",
    "\n",
    "                    presence_list = np.array([0]*len(feat_list))                    \n",
    "\n",
    "                    presence_list[feat_pos] = 1\n",
    "\n",
    "                    feat_pres.append(presence_list)\n",
    "                    feat_weights.append(weights)\n",
    "                \n",
    "                #print(feat_pres)\n",
    "                #print(feat_weights)\n",
    "                \n",
    "                stability = st.getStability(feat_pres)\n",
    "                print (\"Stability:\", round(stability,2))\n",
    "                subset_stability.append(stability)\n",
    "\n",
    "                rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                avg_dispersal = 1-np.mean(rel_var)\n",
    "                print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                weight_stability.append(avg_dispersal)\n",
    "                adj_dispersal = 1-np.mean(second_var)\n",
    "                print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                adjusted_weight_stability.append(adj_dispersal)\n",
    "\n",
    "            results[\"LINDA Subset Stability\"] = subset_stability\n",
    "            results[\"LINDA Weight Stability\"] = weight_stability\n",
    "            results[\"LINDA Adjusted Weight Stability\"] = adjusted_weight_stability\n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                               (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False)                \n",
    "            all_results.append(results)\n",
    "            \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Prefix Length</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction Probability</th>\n",
       "      <th>SHAP Subset Stability</th>\n",
       "      <th>SHAP Weight Stability</th>\n",
       "      <th>SHAP Adjusted Weight Stability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.267326</td>\n",
       "      <td>-12.363663</td>\n",
       "      <td>-12.363663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>950</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.745323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>975</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991057</td>\n",
       "      <td>0.991057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922846</td>\n",
       "      <td>0.922846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186198</td>\n",
       "      <td>-18.916109</td>\n",
       "      <td>-18.916109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>979_3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.922513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>981_9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.398195</td>\n",
       "      <td>-5.172324</td>\n",
       "      <td>-5.172324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>986_12</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623494</td>\n",
       "      <td>-6.615857</td>\n",
       "      <td>-6.615857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>987_7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998441</td>\n",
       "      <td>0.704879</td>\n",
       "      <td>-7.023331</td>\n",
       "      <td>-7.023331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>995_7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.462782</td>\n",
       "      <td>-4.971222</td>\n",
       "      <td>-4.971222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case ID  Actual  Prefix Length  Prediction  Prediction Probability  \\\n",
       "0       914       1              1           1                1.000000   \n",
       "1       950       0              1           0                1.000000   \n",
       "2       975       0              1           0                1.000000   \n",
       "3       999       0              1           0                1.000000   \n",
       "4      1004       1              1           1                1.000000   \n",
       "..      ...     ...            ...         ...                     ...   \n",
       "107   979_3       0              3           0                1.000000   \n",
       "108   981_9       1              9           1                1.000000   \n",
       "109  986_12       1             12           1                1.000000   \n",
       "110   987_7       1              7           1                0.998441   \n",
       "111   995_7       1              7           1                0.999969   \n",
       "\n",
       "     SHAP Subset Stability  SHAP Weight Stability  \\\n",
       "0                 0.267326             -12.363663   \n",
       "1                 1.000000               0.745323   \n",
       "2                 1.000000               0.991057   \n",
       "3                 1.000000               0.922846   \n",
       "4                 0.186198             -18.916109   \n",
       "..                     ...                    ...   \n",
       "107               1.000000               0.922513   \n",
       "108               0.398195              -5.172324   \n",
       "109               0.623494              -6.615857   \n",
       "110               0.704879              -7.023331   \n",
       "111               0.462782              -4.971222   \n",
       "\n",
       "     SHAP Adjusted Weight Stability  \n",
       "0                        -12.363663  \n",
       "1                          0.745323  \n",
       "2                          0.991057  \n",
       "3                          0.922846  \n",
       "4                        -18.916109  \n",
       "..                              ...  \n",
       "107                        0.922513  \n",
       "108                       -5.172324  \n",
       "109                       -6.615857  \n",
       "110                       -7.023331  \n",
       "111                       -4.971222  \n",
       "\n",
       "[112 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_linda_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mget_linda_features\u001b[1;34m(instance, cls, scaler, dataset, exp_iter, feat_list, percentile)\u001b[0m\n\u001b[0;32m      7\u001b[0m save_to \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PATH, dataset, cls_method, method_name)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(exp_iter):\n\u001b[1;32m---> 10\u001b[0m     [bn, inference, infoBN] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_BN_explanations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_in_notebook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     ie \u001b[38;5;241m=\u001b[39m pyAgrum\u001b[38;5;241m.\u001b[39mLazyPropagation(bn)\n\u001b[0;32m     15\u001b[0m     result_posterior \u001b[38;5;241m=\u001b[39m ie\u001b[38;5;241m.\u001b[39mposterior(bn\u001b[38;5;241m.\u001b[39midFromName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mtopandas()\n",
      "File \u001b[1;32m~\\Documents\\full_stability\\learning.py:664\u001b[0m, in \u001b[0;36mgenerate_BN_explanations\u001b[1;34m(instance, label_lst, feature_names, class_var, encoder, scaler, model, path, dataset_name, show_in_notebook, samples)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# Necessary for starting core Python generated random numbers in a state\u001b[39;00m\n\u001b[0;32m    662\u001b[0m rn\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m515\u001b[39m)\n\u001b[1;32m--> 664\u001b[0m indx \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    665\u001b[0m prediction_type \u001b[38;5;241m=\u001b[39m instance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m prediction_type \u001b[38;5;241m=\u001b[39m prediction_type\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "get_linda_features(instance, cls, scaler, dataset_ref, 1, feat_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "feat_pos = []\n",
    "lkhoods = []\n",
    "\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)+\"/\"\n",
    "\n",
    "[bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                       None, scaler, cls, save_to, dataset_ref, show_in_notebook = False,\n",
    "                                                       samples=round(len(feat_list)*1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = pyAgrum.LazyPropagation(bn)\n",
    "result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "\n",
    "if len(result_posterior.shape)==1:\n",
    "    result_proba = result_posterior.values[0]\n",
    "else:\n",
    "    result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "\n",
    "row = instance['original_vector']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "            \n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "            \n",
    "            feat_bin = None\n",
    "            val = row[j]\n",
    "            \n",
    "            #Find appropriate bin, if higher or lower than bins,\n",
    "            #use first or last bin\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and val <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and val >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif val > bins[k][0] and val <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            #If the value doesn't fit into any bin,\n",
    "            #pick the nearest\n",
    "            if feat_bin == None: \n",
    "                bins_diff = np.array(bins) - val\n",
    "                inds = np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)\n",
    "                k = inds[0]\n",
    "                feat_bin = str_bins[k]\n",
    "                \n",
    "#                print(bins_diff)\n",
    "#                 for k in range(len(bins)):\n",
    "#                     if k!=len(bins)-1 and bins[k+1][0]-bins[k][1]!=0:\n",
    "#                         print(\"Gap between bins\")\n",
    "#                         lower = val - bins[k][1]\n",
    "#                         higher = bins[k+1][0] - val\n",
    "#                         if lower > higher:\n",
    "#                             feat_bin = str_bins[k+1]\n",
    "#                         else:\n",
    "#                             feat_bin = str_bins[k]\n",
    "#                 if feat_bin==None:\n",
    "#                     print(\"Some other issue with bins\")\n",
    "                    \n",
    "            print(val)\n",
    "            #print(row[j])\n",
    "            print(str_bins[k])\n",
    "\n",
    "            ie = pyAgrum.LazyPropagation(bn)\n",
    "            ie.setEvidence({feat_list[j]: feat_bin})\n",
    "            ie.makeInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins[k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_diff = np.array(bins) - val\n",
    "print(bins_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_diff = np.array([[6, 3, 1], [2, 5, 6], [2, 0, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.abs(bins_diff).argmin(axis=None), bins_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance[\"original_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict[0][\"scaled_vector\"][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.variable(feat_list[j-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showInference(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(trainingdata, columns=feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAgrum.lib.bn2graph import BN2dot, BNinference2dot\n",
    "\n",
    "g = BNinference2dot(bn, size='\"100,10!\"', engine=ie)\n",
    "\n",
    "g.write(\"bn2graph_test.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_posterior.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = pyAgrum.BNLearner(save_to+\"feature_permutations/bpic2012/false_negatives/0_permutations.csv\")\n",
    "bn = learner.learnBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = all_samples[0]\n",
    "instance = all_samples[0][0]\n",
    "trainingdata = all_train[0]\n",
    "\n",
    "cls = all_cls[0]\n",
    "scaler = all_scalers[0]\n",
    "\n",
    "feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "\n",
    "class_names = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names)\n",
    "if scaler == None:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                                 cls.predict_proba, num_features=10, labels=[0,1], top_labels=1)\n",
    "else:\n",
    "    exp = lime_explainer.explain_instance(instance, \n",
    "                             scale_predict_fn, num_features=max_feat, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(cls)\n",
    "explanation = shap_explainer(testingdata)\n",
    "explanation.feature_names = feat_list\n",
    "shap.plots.waterfall(explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(scaler.inverse_transform(trainingdata), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(dataset_name)\n",
    "dataset_manager.static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.dynamic_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager.static_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols for feat in range(len(feat_list)) if col in feat_list[feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(feat_list)[cats]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
